{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "278faf58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "#necessary imports\n",
    "import cv2\n",
    "import itertools\n",
    "import numpy as np\n",
    "from time import time\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d\n",
    "import PySimpleGUI as sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d30c10f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectFacialLandmarks(image, face_mesh, display = True):\n",
    "    '''\n",
    "    This function performs facial landmarks detection on an image.\n",
    "    Args:\n",
    "        image:     The input image of person(s) whose facial landmarks needs to be detected.\n",
    "        face_mesh: The face landmarks detection function required to perform the landmarks detection.\n",
    "        display:   A boolean value that is if set to true the function displays the original input image, \n",
    "                   and the output image with the face landmarks drawn and returns nothing.\n",
    "    Returns:\n",
    "        output_image: A copy of input image with face landmarks drawn.\n",
    "        results:      The output of the facial landmarks detection on the input image.\n",
    "    '''\n",
    "    \n",
    "    # Perform the facial landmarks detection on the image, after converting it into RGB format.\n",
    "    results = face_mesh.process(image[:,:,::-1])\n",
    "    \n",
    "    # Create a copy of the input image to draw facial landmarks.\n",
    "    output_image = image[:,:,::-1].copy()\n",
    "    \n",
    "    # Check if facial landmarks in the image are found.\n",
    "    if results.multi_face_landmarks:\n",
    "\n",
    "        # Iterate over the found faces.\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "\n",
    "            # Draw the facial landmarks on the output image with the face mesh tesselation\n",
    "            # connections using default face mesh tesselation style.\n",
    "            mp_drawing.draw_landmarks(image=output_image, landmark_list=face_landmarks,\n",
    "                                      connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                                      landmark_drawing_spec=None, \n",
    "                                      connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "\n",
    "            # Draw the facial landmarks on the output image with the face mesh contours\n",
    "            # connections using default face mesh contours style.\n",
    "            mp_drawing.draw_landmarks(image=output_image, landmark_list=face_landmarks,\n",
    "                                      connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                                      landmark_drawing_spec=None, \n",
    "                                      connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())\n",
    "\n",
    "    # Check if the original input image and the output image are specified to be displayed.\n",
    "    if display:\n",
    "        \n",
    "        # Display the original input image and the output image.\n",
    "        plt.figure(figsize=[15,15])\n",
    "        plt.subplot(121);plt.imshow(image[:,:,::-1]);plt.title(\"Original Image\");plt.axis('off');\n",
    "        plt.subplot(122);plt.imshow(output_image);plt.title(\"Output\");plt.axis('off');\n",
    "        \n",
    "    # Otherwise\n",
    "    else:\n",
    "        \n",
    "        # Return the output image in BGR format and results of facial landmarks detection.\n",
    "        return np.ascontiguousarray(output_image[:,:,::-1], dtype=np.uint8), results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09f8b4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSize(image, face_landmarks, INDEXES):\n",
    "    '''\n",
    "    This function calculates the height and width of a face part utilizing its landmarks.\n",
    "    Args:\n",
    "        image:          The image of person(s) whose face part size is to be calculated.\n",
    "        face_landmarks: The detected face landmarks of the person whose face part size is to \n",
    "                        be calculated.\n",
    "        INDEXES:        The indexes of the face part landmarks, whose size is to be calculated.\n",
    "    Returns:\n",
    "        width:     The calculated width of the face part of the face whose landmarks were passed.\n",
    "        height:    The calculated height of the face part of the face whose landmarks were passed.\n",
    "        landmarks: An array of landmarks of the face part whose size is calculated.\n",
    "    '''\n",
    "    \n",
    "    # Retrieve the height and width of the image.\n",
    "    image_height, image_width, _ = image.shape\n",
    "    \n",
    "    # Initialize a list to store the landmarks of the face part.\n",
    "    landmarks = []\n",
    "    \n",
    "    # Iterate over the indexes of the landmarks of the face part. \n",
    "    for INDEX in INDEXES:\n",
    "        \n",
    "        # Append the landmark into the list.\n",
    "        landmarks.append([int(face_landmarks.landmark[INDEX].x * image_width),\n",
    "                               int(face_landmarks.landmark[INDEX].y * image_height)])\n",
    "    \n",
    "    # Calculate the width and height of the face part.\n",
    "    # TODO: Use the 3D box instead of 2D rectangle\n",
    "    _, _, width, height = cv2.boundingRect(np.array(landmarks))\n",
    "    \n",
    "    # Convert the list of landmarks of the face part into a numpy array.\n",
    "    landmarks = np.array(landmarks)\n",
    "    \n",
    "    # Return the calculated width height and the landmarks of the face part.\n",
    "    return width, height, landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1e9884fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def isOpen(image, face_mesh_results, face_part, threshold=5):\n",
    "    '''\n",
    "    This function checks whether the eye or mouth of the person(s) is open, \n",
    "    utilizing its facial landmarks.\n",
    "    Args:\n",
    "        image:             The image of person(s) whose an eye or mouth is to be checked.\n",
    "        face_mesh_results: The output of the facial landmarks detection on the image.\n",
    "        face_part:         The name of the face part that is required to check: MOUTH, RIGHT EYE, LEFT EYE\n",
    "        threshold:         The threshold value used to check the isOpen condition.\n",
    "    Returns:\n",
    "        status:       A dictionary containing isOpen statuses of the face part of all the \n",
    "                      detected faces.  \n",
    "    '''\n",
    "    \n",
    "    # Retrieve the height and width of the image.\n",
    "    image_height, image_width, _ = image.shape\n",
    "\n",
    "    \n",
    "    # Create a dictionary to store the isOpen status of the face part of all the detected faces.\n",
    "    status={}\n",
    "    \n",
    "    # Check if the face part is mouth.\n",
    "    if face_part == 'MOUTH':\n",
    "        \n",
    "        # Get the indexes of the mouth.\n",
    "        INDEXES = mp_face_mesh.FACEMESH_LIPS\n",
    "        \n",
    "    # Check if the face part is left eye.    \n",
    "    elif face_part == 'LEFT EYE':\n",
    "        \n",
    "        # Get the indexes of the left eye.\n",
    "        INDEXES = mp_face_mesh.FACEMESH_LEFT_EYE\n",
    "    \n",
    "    # Check if the face part is right eye.    \n",
    "    elif face_part == 'RIGHT EYE':\n",
    "        \n",
    "        # Get the indexes of the right eye.\n",
    "        INDEXES = mp_face_mesh.FACEMESH_RIGHT_EYE \n",
    "           \n",
    "    # Otherwise return nothing.\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    # Convert the indexes of the landmarks of the face part into a list of unique indices.\n",
    "    INDEXES_LIST = set(list(itertools.chain(*INDEXES)))\n",
    "    \n",
    "    # Iterate over the found faces.\n",
    "    for face_no, face_landmarks in enumerate(face_mesh_results.multi_face_landmarks):\n",
    "        \n",
    "        # Get the height of the face part.\n",
    "        _, height, _ = getSize(image, face_landmarks, INDEXES_LIST)\n",
    "        \n",
    "        # Get the height of the whole face.\n",
    "        face_oval = set(list(itertools.chain(*mp_face_mesh.FACEMESH_FACE_OVAL)))\n",
    "        _, face_height, _ = getSize(image, face_landmarks, face_oval)\n",
    "        \n",
    "        # Check if the face part is open.\n",
    "        if (height/face_height)*100 > threshold:\n",
    "            \n",
    "            # Set status of the face part to open.\n",
    "            status[face_no] = 'OPEN'\n",
    "        \n",
    "        # Otherwise.\n",
    "        else:\n",
    "            # Set status of the face part to close.\n",
    "            status[face_no] = 'CLOSE'\n",
    "    \n",
    "    # Otherwise\n",
    "    else:\n",
    "        \n",
    "        # Return the output image and the isOpen statuses of the face part of each detected face.\n",
    "        return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff4ff481",
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkRotation(face_landmarks):\n",
    "    '''\n",
    "    This function returns the order of the forward rectangle points of the 3D bounding box of the face\n",
    "    Args:\n",
    "        face_landmarks: Landmarks of the face\n",
    "    Returns:\n",
    "        -:      The number represents the order of the forward rectangle points, which are the points 0, 1, 2 and 7 \n",
    "                of the array with the 8 points, in the oriented space\n",
    "                0: 0*****2    1: 7*****1    2: 2*****0    3: 1*****7    4: error\n",
    "                   *     *       *     *       *     *       *     *\n",
    "                   *     *       *     *       *     *       *     *\n",
    "                   1*****7       2*****0       7*****1       0*****2\n",
    "        R: Rotation matrix of the 3D bounding box\n",
    "        points: points of the 3D bounding box\n",
    "    '''\n",
    "    \n",
    "    # Retrieve the landmarks of the face as a list\n",
    "    face_landmarks_l = []\n",
    "    for index in range(len(face_landmarks.landmark)):\n",
    "        face_landmarks_l.append([face_landmarks.landmark[index].x, \n",
    "                          face_landmarks.landmark[index].y, \n",
    "                          face_landmarks.landmark[index].z])\n",
    "    \n",
    "    # Get the 3D oriented bounding box\n",
    "    o3d_landmarks_f = o3d.utility.Vector3dVector(np.array(face_landmarks_l))\n",
    "    o3d_bbox_f = o3d.geometry.OrientedBoundingBox.create_from_points(o3d_landmarks_f)\n",
    "    \n",
    "    # Get the rotation matrix from the oriented space to the aligned space\n",
    "    R = o3d_bbox_f.R\n",
    "    points = np.asarray(o3d_bbox_f.get_box_points())\n",
    "    \n",
    "    # In the aligned space the points are stored like this\n",
    "    #  2*****7\n",
    "    #  *     *\n",
    "    #  *     *\n",
    "    #  0*****1\n",
    "    # So we check how they turned from oriented space to aligned space to get the rotation/inversion\n",
    "    if points[2, 0] > points[1, 0] and points[2, 1] > points[1, 1]:\n",
    "        return 0, R, points\n",
    "    elif points[2, 0] < points[1, 0] and points[2, 1] < points[1, 1]:\n",
    "        return 1, R, points\n",
    "    elif points[2, 0] < points[1, 0] and points[2, 1] > points[1, 1]:\n",
    "        return 2, R, points\n",
    "    elif points[2, 0] > points[1, 0] and points[2, 1] < points[1, 1]:\n",
    "        return 3, R, points\n",
    "    else:\n",
    "        return 4, R, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6cd188b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundingBox(face_landmarks, face_part, INDEXES, angle, R, required_size):\n",
    "    '''\n",
    "    This function returns the new corner points of the filter image in the image space\n",
    "    Args:\n",
    "        face_landmarks: Landmarks of the face\n",
    "        face_part:      Name of the face part we want to apply the filter on\n",
    "        INDEXES:        Indices of the face part landmarks\n",
    "        angle:          Orientation of the 3D bouding box of the entire face\n",
    "        R:              The rotation matrix of the 3D bounding box\n",
    "        required_size:  Required width and height of the filter in the aligned space = before applying persective\n",
    "    Returns:\n",
    "        forward_points: New corner points of the filter image in the oriented space\n",
    "    '''\n",
    "    \n",
    "    # Retrieve the landmarks of the face part\n",
    "    landmarks = []\n",
    "    for index in INDEXES:\n",
    "        landmarks.append([face_landmarks.landmark[index].x, \n",
    "                          face_landmarks.landmark[index].y, \n",
    "                          face_landmarks.landmark[index].z])\n",
    "    \n",
    "    # Create the 3D oriented bounding box\n",
    "    o3d_landmarks = o3d.utility.Vector3dVector(np.array(landmarks))\n",
    "    o3d_bbox = o3d.geometry.OrientedBoundingBox.create_from_points(o3d_landmarks)\n",
    "    \n",
    "    # Get box points and center point\n",
    "    box_points = np.asarray(o3d_bbox.get_box_points())\n",
    "    box_center = np.asarray(o3d_bbox.get_center())\n",
    "\n",
    "    # Retrieve the inverse rotation matrix of the oriented bounding box --> R_inv put a point in the oriented space\n",
    "    # to the aligned space\n",
    "    R_inv = np.linalg.inv(R)\n",
    "    \n",
    "    # Retrieve one of the forward points to get the depth in the aligned space\n",
    "    index = np.argmin(box_points[:,2])\n",
    "    forward_point = box_points[index, :] \n",
    "    \n",
    "    # Project the point in the aligned space and center\n",
    "    forward_point_projected = np.matmul(R_inv, forward_point - box_center)\n",
    "    \n",
    "    # Get the required width and height\n",
    "    required_width, required_height = required_size\n",
    "    \n",
    "\n",
    "    # Get the points position in aligned space in the right order\n",
    "    # It depends on the filter type and orientation of the bounding box\n",
    "    # We are in the aligned space and we will then put the image in the oriented space and then in the image space.\n",
    "    # We want the points such that we have 0*****3 in image space, the number being the order in which they are arranged\n",
    "    #                                      *     * \n",
    "    #                                      *     *\n",
    "    #                                      1*****2\n",
    "    # If angle == 0\n",
    "    # In aligned space we have 2*****7, in oriented space 0*****2, and in image space 0*****1 for the bounding box points\n",
    "    #                          *     *                    *     *                     *     *\n",
    "    #                          *     *                    *     *                     *     *\n",
    "    #                          0*****1                    1*****7                     2*****7\n",
    "    #\n",
    "    # So the the points should be written 2*****3\n",
    "    #                                     *     *\n",
    "    #                                     *     *\n",
    "    #                                     1*****0\n",
    "    # That way we will have \n",
    "    # In aligned space 2*****3, in oriented space 1*****2, and in image space 0*****3\n",
    "    #                  *     *                    *     *                     *     *\n",
    "    #                  *     *                    *     *                     *     *\n",
    "    #                  1*****0                    0*****3                     1*****2\n",
    "    forward_points_transformed = []\n",
    "    if face_part == 'FOREHEAD':        \n",
    "        if angle == 0:            \n",
    "            forward_points_transformed.append([required_height, -required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([0, -required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([0, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([required_height, required_width / 2, forward_point_projected[2]])\n",
    "        elif angle == 1:\n",
    "            forward_points_transformed.append([-required_height, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([0, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([0, - required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([-required_height, - required_width / 2, forward_point_projected[2]])\n",
    "        elif angle == 2:\n",
    "            forward_points_transformed.append([required_height, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([0, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([0, - required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([required_height, - required_width / 2, forward_point_projected[2]])\n",
    "        elif angle == 3:\n",
    "            forward_points_transformed.append([-required_height, -required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([0, -required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([0, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([-required_height, required_width / 2, forward_point_projected[2]])\n",
    "        else:\n",
    "            return\n",
    "\n",
    "            \n",
    "    elif face_part == 'RIGHT EYE' or face_part == 'LEFT EYE':  \n",
    "        if angle == 0:            \n",
    "            forward_points_transformed.append([required_height/2, -required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([-required_height/2, -required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([-required_height/2, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([required_height/2, required_width / 2, forward_point_projected[2]])\n",
    "        elif angle == 1:\n",
    "            forward_points_transformed.append([-required_height/2, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([required_height/2, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([required_height/2, - required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([-required_height/2, - required_width / 2, forward_point_projected[2]])\n",
    "        elif angle == 2:\n",
    "            forward_points_transformed.append([required_height/2, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([-required_height/2, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([-required_height/2, - required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([required_height/2, - required_width / 2, forward_point_projected[2]])\n",
    "        elif angle == 3:\n",
    "            forward_points_transformed.append([-required_height/2, -required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([required_height/2, -required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([required_height/2, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([-required_height/2, required_width / 2, forward_point_projected[2]])\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "    elif face_part == 'MOUTH':\n",
    "        if angle == 0:            \n",
    "            forward_points_transformed.append([0, -required_width / 2, 0])\n",
    "            forward_points_transformed.append([-required_height, -required_width / 2, 0])\n",
    "            forward_points_transformed.append([-required_height, required_width / 2, 0])\n",
    "            forward_points_transformed.append([0, required_width / 2, 0])\n",
    "        elif angle == 1:\n",
    "            forward_points_transformed.append([0, required_width / 2, 0])\n",
    "            forward_points_transformed.append([required_height, required_width / 2, 0])\n",
    "            forward_points_transformed.append([required_height, - required_width / 2, 0])\n",
    "            forward_points_transformed.append([0, - required_width / 2, 0])\n",
    "        elif angle == 2:\n",
    "            forward_points_transformed.append([0, required_width / 2, 0])\n",
    "            forward_points_transformed.append([-required_height, required_width / 2, 0])\n",
    "            forward_points_transformed.append([-required_height, - required_width / 2, 0])\n",
    "            forward_points_transformed.append([0, - required_width / 2, 0])\n",
    "        elif angle == 3:\n",
    "            forward_points_transformed.append([0, -required_width / 2, 0])\n",
    "            forward_points_transformed.append([required_height, -required_width / 2, 0])\n",
    "            forward_points_transformed.append([required_height, required_width / 2, 0])\n",
    "            forward_points_transformed.append([0, required_width / 2, 0])\n",
    "        else:\n",
    "            return     \n",
    "    else:\n",
    "        return\n",
    "       \n",
    "    # Get the filter image in the oriented space \n",
    "    forward_points = []\n",
    "    for p in forward_points_transformed:\n",
    "        p_rotated = np.matmul(R, p) # Rotate\n",
    "        p_translated = p_rotated + box_center # Translate to box position\n",
    "        forward_points.append(p_translated)\n",
    "            \n",
    "    return forward_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c2b9c3dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawFilter(image, filter_img, face_landmarks, face_part, INDEXES, angle, R, required_size):\n",
    "    '''\n",
    "    This function returns camera image with the filter image added at the right place\n",
    "    Args:\n",
    "        image:           Camera image\n",
    "        filter_image:    Filter image\n",
    "        face_landmarks:  Landmarks of the entire face\n",
    "        face_part:       The name of the part you want to put the filter on\n",
    "        INDEXES:         Indices of the face part landmarks\n",
    "        angle:           Orientation of the 3D bouding box of the entire face\n",
    "        R:               The rotation matrix of the 3D bounding box\n",
    "        required_size:   Required width and height of the filter in the aligned space = before applying persective\n",
    "    Returns:\n",
    "        annotated_image: The image with the filter added\n",
    "    '''\n",
    "    \n",
    "    # Get width and height of the filter image\n",
    "    filter_img_height, filter_img_width, _  = filter_img.shape\n",
    "    \n",
    "    # Get width and height of the camera image\n",
    "    image_height, image_width, _ = image.shape\n",
    "    \n",
    "    # Get required width and height\n",
    "    required_width = required_size[0]\n",
    "    required_height = required_size[1]\n",
    "    \n",
    "    # Normalize the required width and height for aligned/oriented spaces\n",
    "    required_height_norm = required_height / image_height\n",
    "    required_width_norm = required_width / image_width\n",
    "    \n",
    "    # Get the new corners points of the filter image (in the oriented space)\n",
    "    forward_points = boundingBox(face_landmarks, face_part, INDEXES, angle, R,\n",
    "                                                             [required_width_norm, required_height_norm])\n",
    "          \n",
    "    # Get the initial corner points of the filter image (counterclockwise) in image space\n",
    "    # 0*****3\n",
    "    # *     *\n",
    "    # *     *\n",
    "    # 1*****2\n",
    "    pts1 = np.array([[0, 0], [0, filter_img_height], \n",
    "                     [filter_img_width, filter_img_height], [filter_img_width, 0]])\n",
    "    \n",
    "    # Get the final corner points of the filter image in the overlay image (counterclockwise) in image space\n",
    "    # with perspective\n",
    "    pts2 = []\n",
    "    for point in forward_points:\n",
    "        resolution_x = 1\n",
    "        resolution_y = 1\n",
    "\n",
    "        # Add perspective\n",
    "        fov = 45\n",
    "        z0 = (resolution_x / 2) / np.tan((fov/2) * np.pi / 180)\n",
    "\n",
    "        relative_x = int(((point[0] - 0.5) * z0 / (z0 + point[2]) + 0.5) * image_width)\n",
    "        relative_y = int(((point[1] - 0.5) * z0 / (z0 + point[2]) + 0.5) * image_height)\n",
    "\n",
    "        pts2.append([relative_x, relative_y])\n",
    "    pts2 = np.array(pts2)\n",
    "\n",
    "    # Find homography from the filter image to the overlay image\n",
    "    h, mask = cv2.findHomography(pts1, pts2, cv2.RANSAC,5.0)\n",
    "\n",
    "    # Put the filter image in perspective\n",
    "    im1Reg = cv2.warpPerspective(filter_img, h, (image_width, image_height))\n",
    "\n",
    "    # Inverse filter image mask in perspective\n",
    "    _, filter_img_mask = cv2.threshold(cv2.cvtColor(im1Reg, cv2.COLOR_BGR2GRAY),\n",
    "                                   25, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    # Spread in 3 dimensions (rgb)\n",
    "    filter_img_mask = np.expand_dims(filter_img_mask, axis=2)\n",
    "    filter_img_mask = np.repeat(filter_img_mask, 3, axis=2)\n",
    "\n",
    "    # Final image with black pixels at filter image position\n",
    "    resultant_image = cv2.bitwise_and(image, filter_img_mask)\n",
    "\n",
    "    # Use Bitwise or to merge the two images\n",
    "    annotated_image = cv2.bitwise_or(im1Reg, resultant_image)        \n",
    "\n",
    "    return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d57c842c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#bgr to hsv function below, the bgr image is only param\n",
    "def bgr2hsv(bgr):\n",
    "  bgr = np.float32(bgr)\n",
    "  b = bgr[:,:,0]\n",
    "  g = bgr[:,:,1]\n",
    "  r = bgr[:,:,2]\n",
    "\n",
    "  rows = bgr.shape[0]\n",
    "  cols = bgr.shape[1]\n",
    "\n",
    "  v = np.maximum(np.maximum(r, g), b) # only compares two arrays at a time\n",
    "  m = np.minimum(np.minimum(r, g), b)\n",
    "  c = np.subtract(v, m)\n",
    "\n",
    "  s = np.divide(c, v, np.zeros_like(c), where=v!=0)\n",
    "  h = []\n",
    "\n",
    "  conditions = [v == r, v == g, v == b, np.array_equal(r, g) and np.array_equal(g, b)]\n",
    "  choices = [60*np.divide((g-b), c, np.zeros_like(c), where=c!=0), 120+60*np.divide((b-r), c, np.zeros_like(c), where=c!=0), 240+60*np.divide((r-g), c, np.zeros_like(c), where=c!=0), 0]\n",
    "  h = np.select(conditions, choices)\n",
    "\n",
    "  h[h<0] += 360\n",
    "\n",
    "  return np.uint8(np.array([h/2,s*255,v])).transpose(1,2,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "599de949",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functin to find lip connections below, no param\n",
    "def findLipConnections():\n",
    "    lipConnections = [0, 267, 269, 270, 409, 291, 375, 321, 405, 314, 17, 84, 181, 91, 146, 61, 185, 40, 39, 37, 0, 13, 82, 81, 80, 191, 78, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308, 415, 310, 311, 312, 13, 0]\n",
    "    return lipConnections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c621e24b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the mediapipe face mesh class.\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "# Setup the face landmarks function for videos.\n",
    "# TODO: Here works for only one face --> Make it work for multiple faces (see overlay call in main function)\n",
    "face_mesh_videos = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, \n",
    "                                         min_detection_confidence=0.5,min_tracking_confidence=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7db1adf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the mediapipe drawing class.\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize the mediapipe drawing styles class.\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013187a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#use a blue theme for the GUI\n",
    "sg.theme(\"LightBlue7\")\n",
    "    \n",
    "# Define the window layout for the GUI\n",
    "layout = [\n",
    "    [sg.Image(filename=\"\", key=\"-IMAGE-\")],\n",
    "    [sg.Radio(\"None\", \"Filter\", True, size=(6, 1), key=\"NONE\"),\n",
    "     sg.Radio(\"Perspective Filter\", \"Filter\", size=(15, 1), key=\"QUEEN\"),\n",
    "     sg.Radio(\"Lip Filter\", \"Filter\", size=(10, 1), key=\"LIPSTICK\")],\n",
    "]\n",
    "\n",
    "# Create the window and show it without the plot for the GUI\n",
    "window = sg.Window(\"SNAPCHAT FILTER CAMERA\", layout, location=(800, 400))\n",
    "\n",
    "# Initialize the VideoCapture object to read from the webcam.\n",
    "camera_video = cv2.VideoCapture(0)\n",
    "\n",
    "# Set camera resolution\n",
    "camera_video.set(3,1280)\n",
    "camera_video.set(4,960)\n",
    "\n",
    "# Read the left and right eyes images.\n",
    "# TODO: Create an interactive window to let the user select the wanted filters\n",
    "left_eye = cv2.imread('data/left_eye_cupcake.png')\n",
    "right_eye = cv2.imread('data/right_eye_cupcake.png')\n",
    "\n",
    "crown = cv2.imread('data/crown.png')\n",
    "\n",
    "# Initialize the VideoCapture object to read from the smoke animation video stored in the disk.\n",
    "# TODO: Same as for images --> Create a dictionary to store all images and videos?\n",
    "animation = cv2.VideoCapture('data/rainbow_animation1.mp4')\n",
    "\n",
    "# Set the smoke animation video frame counter to zero.\n",
    "animation_frame_counter = 0\n",
    "\n",
    "# Iterate until the webcam is accessed successfully.\n",
    "while camera_video.isOpened():\n",
    "    event, values = window.read(timeout=20)\n",
    "    if event == sg.WIN_CLOSED:\n",
    "        break\n",
    "    \n",
    "    # Read a frame.\n",
    "    ok, frame = camera_video.read()\n",
    "    \n",
    "    # Check if frame is not read properly then continue to the next iteration to read\n",
    "    # the next frame.\n",
    "    if not ok:\n",
    "        continue \n",
    "    \n",
    "    #insert perspective code here\n",
    "    # Get the width and height of the image\n",
    "    image_height, image_width, _ = frame.shape\n",
    "    \n",
    "    # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    #if Perspective Filter radio button selected, display the perspective queen filter\n",
    "    if values[\"QUEEN\"]: \n",
    "        # Read a frame from smoke animation video\n",
    "        _, animation_frame = animation.read()\n",
    "    \n",
    "        # Increment the animation video frame counter.\n",
    "        animation_frame_counter += 1\n",
    "\n",
    "        # Check if the current frame is the last frame of the animation video.\n",
    "        if animation_frame_counter == animation.get(cv2.CAP_PROP_FRAME_COUNT):     \n",
    "\n",
    "            # Set the current frame position to first frame to restart the video.\n",
    "            animation.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "\n",
    "            # Set the animation video frame counter to zero.\n",
    "            animation_frame_counter = 0\n",
    "\n",
    "        # Perform Face landmarks detection.\n",
    "        face_mesh_results = face_mesh_videos.process(frame[:,:,::-1])    \n",
    "\n",
    "        # Check if facial landmarks are found.\n",
    "        if face_mesh_results.multi_face_landmarks:        \n",
    "            # Get the mouth isOpen status of the person in the frame.\n",
    "            mouth_status = isOpen(frame, face_mesh_results, 'MOUTH', \n",
    "                                         threshold=15)\n",
    "\n",
    "            # Get the left eye isOpen status of the person in the frame.\n",
    "            left_eye_status = isOpen(frame, face_mesh_results, 'LEFT EYE', \n",
    "                                            threshold=4.5)\n",
    "\n",
    "            # Get the right eye isOpen status of the person in the frame.\n",
    "            right_eye_status = isOpen(frame, face_mesh_results, 'RIGHT EYE', \n",
    "                                             threshold=4.5)\n",
    "\n",
    "            # Iterate over the found faces.\n",
    "            for face_num, face_landmarks in enumerate(face_mesh_results.multi_face_landmarks):\n",
    "\n",
    "                # Check the orientation of the face bounding box\n",
    "                angle, R, points = checkRotation(face_landmarks)\n",
    "\n",
    "                # Check if the left eye of the face is open.\n",
    "                if left_eye_status[face_num] == 'OPEN':\n",
    "\n",
    "                    # Get the width and height of filter image.\n",
    "                    filter_img_height, filter_img_width, _  = left_eye.shape\n",
    "\n",
    "                    # Convert the indexes of the landmarks of the face part into a list.\n",
    "                    left_eye_landmarks = set(list(itertools.chain(*mp_face_mesh.FACEMESH_LEFT_EYE)))\n",
    "\n",
    "                    # Get the height of the face part on which we will overlay the filter image.\n",
    "                    _, face_part_height, landmarks = getSize(frame, face_landmarks, left_eye_landmarks)\n",
    "\n",
    "                    # Specify the height to which the filter image is required to be resized.\n",
    "                    # 2.5 can be changed depending on the size of the filter we want\n",
    "                    # This allows the filter to be bigger/smaller depending on both the size of the eye \n",
    "                    # and the size of the aperture of the eye\n",
    "                    required_height = int(face_part_height*2.5)\n",
    "                    required_width = int(filter_img_width*(required_height/filter_img_height))\n",
    "\n",
    "                    # Draw the filter\n",
    "                    frame = drawFilter(frame, left_eye, face_landmarks, 'LEFT EYE',\n",
    "                                       left_eye_landmarks, angle, R, [required_width, required_height])\n",
    "\n",
    "\n",
    "                # Check if the right eye of the face is open.\n",
    "                if right_eye_status[face_num] == 'OPEN':\n",
    "\n",
    "                    # Get the width and height of filter image.\n",
    "                    filter_img_height, filter_img_width, _  = right_eye.shape\n",
    "\n",
    "                    # Convert the indexes of the landmarks of the face part into a list.\n",
    "                    right_eye_landmarks = set(list(itertools.chain(*mp_face_mesh.FACEMESH_RIGHT_EYE)))\n",
    "\n",
    "                    # Get the height of the face part on which we will overlay the filter image.\n",
    "                    _, face_part_height, landmarks = getSize(frame, face_landmarks, right_eye_landmarks)\n",
    "\n",
    "                    # Specify the height to which the filter image is required to be resized.\n",
    "                    # 2.5 can be changed depending on the size of the filter we want\n",
    "                    # This allows the filter to be bigger/smaller depending on both the size of the eye \n",
    "                    # and the size of the aperture of the eye\n",
    "                    required_height = int(face_part_height*2.5)\n",
    "                    required_width = int(filter_img_width*(required_height/filter_img_height))\n",
    "\n",
    "                    # Draw the filter\n",
    "                    frame = drawFilter(frame, right_eye, face_landmarks, 'RIGHT EYE',\n",
    "                                       right_eye_landmarks, angle, R, [required_width, required_height])\n",
    "\n",
    "                # Check if the mouth of the face is open.\n",
    "                if mouth_status[face_num] == 'OPEN':\n",
    "\n",
    "                    # Get the width and height of filter image.\n",
    "                    filter_img_height, filter_img_width, _  = animation_frame.shape\n",
    "\n",
    "                    # Convert the indexes of the landmarks of the face part into a list.\n",
    "                    lips_landmarks = set(list(itertools.chain(*mp_face_mesh.FACEMESH_LIPS)))\n",
    "\n",
    "                    # Get the height of the face part on which we will overlay the filter image.\n",
    "                    face_part_width, face_part_height, landmarks = getSize(frame, face_landmarks, lips_landmarks)\n",
    "\n",
    "                    # Specify the height to which the filter image is required to be resized.\n",
    "                    # 0.7 can be changed depending on the size of the filter we want\n",
    "                    required_width = int(face_part_width*0.7)\n",
    "                    required_height = int(filter_img_height*(required_width/filter_img_width))\n",
    "\n",
    "                    # Draw the filter\n",
    "                    frame = drawFilter(frame, animation_frame, face_landmarks, 'MOUTH', \n",
    "                                       lips_landmarks, angle, R, [required_width, required_height])\n",
    "\n",
    "                # FOREHEAD\n",
    "                # Get the width and height of filter image.\n",
    "                filter_img_height, filter_img_width, _  = crown.shape\n",
    "\n",
    "                # Get the landmarks of the forehead\n",
    "                forehead_landmarks = [103, 67, 109, 10, 338, 297, 332]\n",
    "\n",
    "                # Get the height of the face part on which we will overlay the filter image.\n",
    "                face_part_width,_, landmarks = getSize(frame, face_landmarks, forehead_landmarks)\n",
    "\n",
    "                # Specify the height to which the filter image is required to be resized.\n",
    "                # 2.5 can be changed depending on the size of the filter we want\n",
    "                required_width = int(face_part_width*2.5)\n",
    "                required_height = int(filter_img_height*(required_width/filter_img_width))\n",
    "\n",
    "                # Draw the filter\n",
    "                frame = drawFilter(frame, crown, face_landmarks, 'FOREHEAD',\n",
    "                                       forehead_landmarks, angle, R, [required_width, required_height])\n",
    "    #if the Lip Filler Filter radio button selected, display the lip filler filter\n",
    "    elif values[\"LIPSTICK\"]:  \n",
    "        window_width = frame.shape[1]\n",
    "        window_height = frame.shape[0]\n",
    "        \n",
    "        # Perform Face landmarks detection.\n",
    "        _, face_mesh_results = detectFacialLandmarks(frame, face_mesh_videos, display=False)\n",
    "\n",
    "        # Check if facial landmarks are found.\n",
    "        if face_mesh_results.multi_face_landmarks:\n",
    "            #list to hold all landmarks\n",
    "            landmarks_all = [];\n",
    "\n",
    "            # Iterate over the found faces.\n",
    "            for face_num, face_landmarks in enumerate(face_mesh_results.multi_face_landmarks):\n",
    "                #draw all landmarks\n",
    "                for i in range(0, len(face_landmarks.landmark)):      \n",
    "                    landmark_x = face_landmarks.landmark[i].x * window_width\n",
    "                    landmark_y = face_landmarks.landmark[i].y * window_height\n",
    "\n",
    "                    #add all landmarks in screen position to landmarks_all list\n",
    "                    landmarks_all.append([int(landmark_x), int(landmark_y)])\n",
    "\n",
    "                #draw mouth landmarks\n",
    "                lips_list = np.zeros((len(mp_face_mesh.FACEMESH_LIPS), 2), np.int32)\n",
    "\n",
    "                #empty list to hold points for lips\n",
    "                lipPoints = []\n",
    "\n",
    "                #add to list of lip points based on lip landmarks\n",
    "                for x, y in mp_face_mesh.FACEMESH_LIPS:\n",
    "                    duo = (x, y)\n",
    "                    lipPoints.append(duo)\n",
    "\n",
    "                #sort the lipPoints to be in order\n",
    "                sorted_lipPoints = sorted(lipPoints, key=lambda x: x[-1])\n",
    "\n",
    "                #color for the chola inspired lipstick\n",
    "                lip_outline_color = (107, 32, 100);\n",
    "\n",
    "                #get points to fill lips\n",
    "                lipPoints_polygon = findLipConnections();\n",
    "                lipPoints_toFill = [] \n",
    "\n",
    "                #get landmark positions to fill lips\n",
    "                for lipPoint in lipPoints_polygon:\n",
    "                    lipPoints_toFill.append(landmarks_all[lipPoint])\n",
    "\n",
    "                #make into numpy array\n",
    "                lipPoints_toFill_np = np.array(lipPoints_toFill, dtype=np.int32)\n",
    "\n",
    "                #go through each lipPoint to draw a line to connect them\n",
    "                for index, lipPoint in enumerate(sorted_lipPoints):\n",
    "                    x_point = lipPoint[0]\n",
    "                    y_point = lipPoint[1]\n",
    "\n",
    "                    #get each start and end point of the landmark in screen space\n",
    "                    start_point = landmarks_all[x_point]\n",
    "                    end_point = landmarks_all[y_point]\n",
    "\n",
    "                    #draw a line connecting all of the lip points, gives us a chola lip inspired look\n",
    "                    #-> currently not displayed\n",
    "#                     frame = cv2.line(frame, start_point, end_point, lip_outline_color, 4)\n",
    "\n",
    "                #color for the filled lipstick -> currently not displayed\n",
    "    #             fill = (255, 255, 255);\n",
    "\n",
    "                # simple fill in the lips like lipstick -> currently not displayed\n",
    "    #             frame = cv2.fillPoly(frame, pts=np.int32([lipPoints_toFill_np]), color =(0,50,255, )) \n",
    "\n",
    "                # eclectic fill of lips -> get mask of lips first\n",
    "                mask = np.zeros(frame.shape[:2], dtype=\"uint8\")\n",
    "                mask = cv2.fillPoly(mask, pts=np.int32([lipPoints_toFill_np]), color =(255,255,255)) \n",
    "                #make copy of frame to just get the pixels of lips\n",
    "                frame_copy = frame.copy()\n",
    "                frame_copy = cv2.bitwise_and(frame_copy, frame, mask=mask)\n",
    "                #make lips color change\n",
    "                frame_copy = bgr2hsv(frame_copy)\n",
    "                #output the masked frame over the video frame\n",
    "                frame = cv2.bitwise_or(frame_copy, frame)\n",
    "\n",
    "\n",
    "\n",
    "    #             #this line below can be used to achieve the lip lined look with mediapipe's own function draw landmarks\n",
    "    #             mp_drawing.draw_landmarks(image=frame, landmark_list=face_landmarks,\n",
    "    #                                       connections=mp_face_mesh.FACEMESH_LIPS,\n",
    "    #                                       landmark_drawing_spec=None, \n",
    "    #                                       connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())\n",
    "\n",
    "    #display the frame with the GUI\n",
    "    imgbytes = cv2.imencode(\".png\", frame)[1].tobytes()\n",
    "    window[\"-IMAGE-\"].update(data=imgbytes)\n",
    "\n",
    "window.close()\n",
    "    \n",
    "# Release the VideoCapture Object and close the windows.                  \n",
    "camera_video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9095e186",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
