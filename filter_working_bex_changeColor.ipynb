{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import itertools\n",
    "import numpy as np\n",
    "from time import time\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the mediapipe face detection class.\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "\n",
    "# Setup the face detection function.\n",
    "face_detection = mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5)\n",
    "\n",
    "# Initialize the mediapipe drawing class.\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectFacialLandmarks(image, face_mesh, display = True):\n",
    "    '''\n",
    "    This function performs facial landmarks detection on an image.\n",
    "    Args:\n",
    "        image:     The input image of person(s) whose facial landmarks needs to be detected.\n",
    "        face_mesh: The face landmarks detection function required to perform the landmarks detection.\n",
    "        display:   A boolean value that is if set to true the function displays the original input image, \n",
    "                   and the output image with the face landmarks drawn and returns nothing.\n",
    "    Returns:\n",
    "        output_image: A copy of input image with face landmarks drawn.\n",
    "        results:      The output of the facial landmarks detection on the input image.\n",
    "    '''\n",
    "    \n",
    "    # Perform the facial landmarks detection on the image, after converting it into RGB format.\n",
    "    results = face_mesh.process(image[:,:,::-1])\n",
    "    \n",
    "    # Create a copy of the input image to draw facial landmarks.\n",
    "    output_image = image[:,:,::-1].copy()\n",
    "    \n",
    "    # Check if facial landmarks in the image are found.\n",
    "    if results.multi_face_landmarks:\n",
    "\n",
    "        # Iterate over the found faces.\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "\n",
    "            # Draw the facial landmarks on the output image with the face mesh tesselation\n",
    "            # connections using default face mesh tesselation style.\n",
    "            mp_drawing.draw_landmarks(image=output_image, landmark_list=face_landmarks,\n",
    "                                      connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                                      landmark_drawing_spec=None, \n",
    "                                      connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "\n",
    "            # Draw the facial landmarks on the output image with the face mesh contours\n",
    "            # connections using default face mesh contours style.\n",
    "            mp_drawing.draw_landmarks(image=output_image, landmark_list=face_landmarks,\n",
    "                                      connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                                      landmark_drawing_spec=None, \n",
    "                                      connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())\n",
    "\n",
    "    # Check if the original input image and the output image are specified to be displayed.\n",
    "    if display:\n",
    "        \n",
    "        # Display the original input image and the output image.\n",
    "        plt.figure(figsize=[15,15])\n",
    "        plt.subplot(121);plt.imshow(image[:,:,::-1]);plt.title(\"Original Image\");plt.axis('off');\n",
    "        plt.subplot(122);plt.imshow(output_image);plt.title(\"Output\");plt.axis('off');\n",
    "        \n",
    "    # Otherwise\n",
    "    else:\n",
    "        \n",
    "        # Return the output image in BGR format and results of facial landmarks detection.\n",
    "        return np.ascontiguousarray(output_image[:,:,::-1], dtype=np.uint8), results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSize(image, face_landmarks, INDEXES):\n",
    "    '''\n",
    "    This function calculate the height and width of a face part utilizing its landmarks.\n",
    "    Args:\n",
    "        image:          The image of person(s) whose face part size is to be calculated.\n",
    "        face_landmarks: The detected face landmarks of the person whose face part size is to \n",
    "                        be calculated.\n",
    "        INDEXES:        The indexes of the face part landmarks, whose size is to be calculated.\n",
    "    Returns:\n",
    "        width:     The calculated width of the face part of the face whose landmarks were passed.\n",
    "        height:    The calculated height of the face part of the face whose landmarks were passed.\n",
    "        landmarks: An array of landmarks of the face part whose size is calculated.\n",
    "    '''\n",
    "    \n",
    "    # Retrieve the height and width of the image.\n",
    "    image_height, image_width, _ = image.shape\n",
    "    \n",
    "    # Convert the indexes of the landmarks of the face part into a list.\n",
    "    INDEXES_LIST = list(itertools.chain(*INDEXES))\n",
    "    \n",
    "    # Initialize a list to store the landmarks of the face part.\n",
    "    landmarks = []\n",
    "    \n",
    "    # Iterate over the indexes of the landmarks of the face part. \n",
    "    for INDEX in INDEXES_LIST:\n",
    "        \n",
    "        # Append the landmark into the list.\n",
    "        landmarks.append([int(face_landmarks.landmark[INDEX].x * image_width),\n",
    "                               int(face_landmarks.landmark[INDEX].y * image_height)])\n",
    "    \n",
    "    # Calculate the width and height of the face part.\n",
    "    _, _, width, height = cv2.boundingRect(np.array(landmarks))\n",
    "    \n",
    "    # Convert the list of landmarks of the face part into a numpy array.\n",
    "    landmarks = np.array(landmarks)\n",
    "    \n",
    "    # Retrurn the calculated width height and the landmarks of the face part.\n",
    "    return width, height, landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isOpen(image, face_mesh_results, face_part, threshold=5, display=True):\n",
    "    '''\n",
    "    This function checks whether the an eye or mouth of the person(s) is open, \n",
    "    utilizing its facial landmarks.\n",
    "    Args:\n",
    "        image:             The image of person(s) whose an eye or mouth is to be checked.\n",
    "        face_mesh_results: The output of the facial landmarks detection on the image.\n",
    "        face_part:         The name of the face part that is required to check.\n",
    "        threshold:         The threshold value used to check the isOpen condition.\n",
    "        display:           A boolean value that is if set to true the function displays \n",
    "                           the output image and returns nothing.\n",
    "    Returns:\n",
    "        output_image: The image of the person with the face part is opened  or not status written.\n",
    "        status:       A dictionary containing isOpen statuses of the face part of all the \n",
    "                      detected faces.  \n",
    "    '''\n",
    "    \n",
    "    # Retrieve the height and width of the image.\n",
    "    image_height, image_width, _ = image.shape\n",
    "    \n",
    "    # Create a copy of the input image to write the isOpen status.\n",
    "    output_image = image.copy()\n",
    "    \n",
    "    # Create a dictionary to store the isOpen status of the face part of all the detected faces.\n",
    "    status={}\n",
    "    \n",
    "    # Check if the face part is mouth.\n",
    "    if face_part == 'MOUTH':\n",
    "        \n",
    "        # Get the indexes of the mouth.\n",
    "        INDEXES = mp_face_mesh.FACEMESH_LIPS\n",
    "        \n",
    "        # Specify the location to write the is mouth open status.\n",
    "        loc = (10, image_height - image_height//40)\n",
    "        \n",
    "        # Initialize a increment that will be added to the status writing location, \n",
    "        # so that the statuses of two faces donot overlap. \n",
    "        increment=-30\n",
    "        \n",
    "    # Check if the face part is left eye.    \n",
    "    elif face_part == 'LEFT EYE':\n",
    "        \n",
    "        # Get the indexes of the left eye.\n",
    "        INDEXES = mp_face_mesh.FACEMESH_LEFT_EYE\n",
    "        \n",
    "        # Specify the location to write the is left eye open status.\n",
    "        loc = (10, 30)\n",
    "        \n",
    "        # Initialize a increment that will be added to the status writing location, \n",
    "        # so that the statuses of two faces donot overlap.\n",
    "        increment=30\n",
    "    \n",
    "    # Check if the face part is right eye.    \n",
    "    elif face_part == 'RIGHT EYE':\n",
    "        \n",
    "        # Get the indexes of the right eye.\n",
    "        INDEXES = mp_face_mesh.FACEMESH_RIGHT_EYE \n",
    "        \n",
    "        # Specify the location to write the is right eye open status.\n",
    "        loc = (image_width-300, 30)\n",
    "        \n",
    "        # Initialize a increment that will be added to the status writing location, \n",
    "        # so that the statuses of two faces donot overlap.\n",
    "        increment=30\n",
    "    \n",
    "    # Otherwise return nothing.\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    # Iterate over the found faces.\n",
    "    for face_no, face_landmarks in enumerate(face_mesh_results.multi_face_landmarks):\n",
    "        \n",
    "         # Get the height of the face part.\n",
    "        _, height, _ = getSize(image, face_landmarks, INDEXES)\n",
    "        \n",
    "         # Get the height of the whole face.\n",
    "        _, face_height, _ = getSize(image, face_landmarks, mp_face_mesh.FACEMESH_FACE_OVAL)\n",
    "        \n",
    "        # Check if the face part is open.\n",
    "        #if (height/face_height)*100 &gt; threshold:\n",
    "        if (height/face_height)*100 > threshold:\n",
    "            \n",
    "            # Set status of the face part to open.\n",
    "            status[face_no] = 'OPEN'\n",
    "            \n",
    "            # Set color which will be used to write the status to green.\n",
    "            color=(0,255,0)\n",
    "        \n",
    "        # Otherwise.\n",
    "        else:\n",
    "            # Set status of the face part to close.\n",
    "            status[face_no] = 'CLOSE'\n",
    "            \n",
    "            # Set color which will be used to write the status to red.\n",
    "            color=(0,0,255)\n",
    "        \n",
    "        # Write the face part isOpen status on the output image at the appropriate location.\n",
    "        cv2.putText(output_image, f'FACE {face_no+1} {face_part} {status[face_no]}.', \n",
    "                    (loc[0],loc[1]+(face_no*increment)), cv2.FONT_HERSHEY_PLAIN, 1.4, color, 2)\n",
    "                \n",
    "    # Check if the output image is specified to be displayed.\n",
    "    if display:\n",
    "\n",
    "        # Display the output image.\n",
    "        plt.figure(figsize=[10,10])\n",
    "        plt.imshow(output_image[:,:,::-1]);plt.title(\"Output Image\");plt.axis('off');\n",
    "    \n",
    "    # Otherwise\n",
    "    else:\n",
    "        \n",
    "        # Return the output image and the isOpen statuses of the face part of each detected face.\n",
    "        return output_image, status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay(image, filter_img, face_landmarks, face_part, INDEXES, display=True):\n",
    "    '''\n",
    "    This function will overlay a filter image over a face part of a person in the image/frame.\n",
    "    Args:\n",
    "        image:          The image of a person on which the filter image will be overlayed.\n",
    "        filter_img:     The filter image that is needed to be overlayed on the image of the person.\n",
    "        face_landmarks: The facial landmarks of the person in the image.\n",
    "        face_part:      The name of the face part on which the filter image will be overlayed.\n",
    "        INDEXES:        The indexes of landmarks of the face part.\n",
    "        display:        A boolean value that is if set to true the function displays \n",
    "                        the annotated image and returns nothing.\n",
    "    Returns:\n",
    "        annotated_image: The image with the overlayed filter on the top of the specified face part.\n",
    "    '''\n",
    "    \n",
    "    # Create a copy of the image to overlay filter image on.\n",
    "    annotated_image = image.copy()\n",
    "    \n",
    "    # Errors can come when it resizes the filter image to a too small or a too large size .\n",
    "    # So use a try block to avoid application crashing.\n",
    "    try:\n",
    "    \n",
    "        # Get the width and height of filter image.\n",
    "        filter_img_height, filter_img_width, _  = filter_img.shape\n",
    "\n",
    "        # Get the height of the face part on which we will overlay the filter image.\n",
    "        _, face_part_height, landmarks = getSize(image, face_landmarks, INDEXES)\n",
    "        \n",
    "        # Specify the height to which the filter image is required to be resized.\n",
    "        required_height = int(face_part_height*2.5)\n",
    "        \n",
    "        # Resize the filter image to the required height, while keeping the aspect ratio constant. \n",
    "        resized_filter_img = cv2.resize(filter_img, (int(filter_img_width*\n",
    "                                                         (required_height/filter_img_height)),\n",
    "                                                     required_height))\n",
    "        \n",
    "        # Get the new width and height of filter image.\n",
    "        filter_img_height, filter_img_width, _  = resized_filter_img.shape\n",
    "\n",
    "        # Convert the image to grayscale and apply the threshold to get the mask image.\n",
    "        _, filter_img_mask = cv2.threshold(cv2.cvtColor(resized_filter_img, cv2.COLOR_BGR2GRAY),\n",
    "                                           25, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "        # Calculate the center of the face part.\n",
    "        center = landmarks.mean(axis=0).astype(\"int\")\n",
    "\n",
    "        # Check if the face part is mouth.\n",
    "        if face_part == 'MOUTH':\n",
    "\n",
    "            # Calculate the location where the smoke filter will be placed.  \n",
    "            location = (int(center[0] - filter_img_width / 3), int(center[1]))\n",
    "\n",
    "        # Otherwise if the face part is an eye.\n",
    "        else:\n",
    "\n",
    "            # Calculate the location where the eye filter image will be placed.  \n",
    "            location = (int(center[0]-filter_img_width/2), int(center[1]-filter_img_height/2))\n",
    "\n",
    "        # Retrieve the region of interest from the image where the filter image will be placed.\n",
    "        ROI = image[location[1]: location[1] + filter_img_height,\n",
    "                    location[0]: location[0] + filter_img_width]\n",
    "\n",
    "        # Perform Bitwise-AND operation. This will set the pixel values of the region where,\n",
    "        # filter image will be placed to zero.\n",
    "        resultant_image = cv2.bitwise_and(ROI, ROI, mask=filter_img_mask)\n",
    "\n",
    "        # Add the resultant image and the resized filter image.\n",
    "        # This will update the pixel values of the resultant image at the indexes where \n",
    "        # pixel values are zero, to the pixel values of the filter image.\n",
    "        resultant_image = cv2.add(resultant_image, resized_filter_img)\n",
    "\n",
    "        # Update the image's region of interest with resultant image.\n",
    "        annotated_image[location[1]: location[1] + filter_img_height,\n",
    "                        location[0]: location[0] + filter_img_width] = resultant_image\n",
    "            \n",
    "    # Catch and handle the error(s).\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    \n",
    "    # Check if the annotated image is specified to be displayed.\n",
    "    if display:\n",
    "\n",
    "        # Display the annotated image.\n",
    "        plt.figure(figsize=[10,10])\n",
    "        plt.imshow(annotated_image[:,:,::-1]);plt.title(\"Output Image\");plt.axis('off');\n",
    "    \n",
    "    # Otherwise\n",
    "    else:\n",
    "            \n",
    "        # Return the annotated image.\n",
    "        return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findLipConnections():\n",
    "    lipConnections = [0, 267, 269, 270, 409, 291, 375, 321, 405, 314, 17, 84, 181, 91, 146, 61, 185, 40, 39, 37, 0, 13, 82, 81, 80, 191, 78, 95, 88, 178, 87, 14, 317, 402, 318, 324, 308, 415, 310, 311, 312, 13, 0]\n",
    "    return lipConnections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bgr2rgb(image):\n",
    "  return image[ :, :, [2, 1, 0] ] # bgr 0,1,2 -> rgb 2,1,0\n",
    "# cv2_imshow(bgr2rgb(imageBGR))\n",
    "\n",
    "def bgr2hsv(bgr):\n",
    "  bgr = np.float32(bgr)\n",
    "  b = bgr[:,:,0]\n",
    "  g = bgr[:,:,1]\n",
    "  r = bgr[:,:,2]\n",
    "\n",
    "  rows = bgr.shape[0]\n",
    "  cols = bgr.shape[1]\n",
    "\n",
    "  v = np.maximum(np.maximum(r, g), b) # only compares two arrays at a time\n",
    "  m = np.minimum(np.minimum(r, g), b)\n",
    "  c = np.subtract(v, m)\n",
    "\n",
    "  s = np.divide(c, v, np.zeros_like(c), where=v!=0)\n",
    "  h = []\n",
    "\n",
    "  conditions = [v == r, v == g, v == b, np.array_equal(r, g) and np.array_equal(g, b)]\n",
    "  choices = [60*np.divide((g-b), c, np.zeros_like(c), where=c!=0), 120+60*np.divide((b-r), c, np.zeros_like(c), where=c!=0), 240+60*np.divide((r-g), c, np.zeros_like(c), where=c!=0), 0]\n",
    "  h = np.select(conditions, choices)\n",
    "\n",
    "  h[h<0] += 360\n",
    "\n",
    "  return np.uint8(np.array([h/2,s*255,v])).transpose(1,2,0)\n",
    "\n",
    "# imageHSV = bgr2hsv(imageBGR)\n",
    "# cv2_imshow(imageHSV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_y_diff(img):\n",
    "      img = img.astype(float)\n",
    "      return img[2:, 1:-1]-img[:-2, 1:-1] #(i+1)-(i-1) along y direction\n",
    "\n",
    "def gradient_x_diff(img):\n",
    "      img = img.astype(float)\n",
    "      return img[1:-1, 2:]-img[1:-1, :-2] #(i+1)-(i-1) along x direction\n",
    "\n",
    "# gx_diff = gradient_x_diff(imageGray)/2\n",
    "# gy_diff = gradient_y_diff(imageGray)/2\n",
    "# gx2_diff = gx_diff ** 2\n",
    "# gy2_diff = gy_diff ** 2\n",
    "# g2_diff = gx2_diff + gy2_diff\n",
    "# imshow([gx2_diff, gy2_diff, g2_diff], ['gx2_diff', 'gy2_diff', 'g2_diff'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the mediapipe face mesh class.\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "# Setup the face landmarks function for images.\n",
    "face_mesh_images = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=2,\n",
    "                                         min_detection_confidence=0.5)\n",
    "\n",
    "# Setup the face landmarks function for videos.\n",
    "face_mesh_videos = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, \n",
    "                                         min_detection_confidence=0.5,min_tracking_confidence=0.3)\n",
    "\n",
    "# Initialize the mediapipe drawing styles class.\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the VideoCapture object to read from the webcam.\n",
    "camera_video = cv2.VideoCapture(0)\n",
    "camera_video.set(3,1280)\n",
    "camera_video.set(4,960)\n",
    "\n",
    "# Create named window for resizing purposes.\n",
    "cv2.namedWindow('Face Filter', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Iterate until the webcam is accessed successfully.\n",
    "while camera_video.isOpened():\n",
    "    \n",
    "    # Read a frame.\n",
    "    ok, frame = camera_video.read()\n",
    "    \n",
    "    # Check if frame is not read properly then continue to the next iteration to read\n",
    "    # the next frame.\n",
    "    if not ok:\n",
    "        continue\n",
    "    \n",
    "    # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    window_width = frame.shape[1]\n",
    "    window_height = frame.shape[0]\n",
    "    \n",
    "    # Perform Face landmarks detection.\n",
    "    _, face_mesh_results = detectFacialLandmarks(frame, face_mesh_videos, display=False)\n",
    "    \n",
    "    # Check if facial landmarks are found.\n",
    "    if face_mesh_results.multi_face_landmarks:\n",
    "        #list to hold all landmarks\n",
    "        landmarks_all = [];\n",
    "        \n",
    "        # Iterate over the found faces.\n",
    "        for face_num, face_landmarks in enumerate(face_mesh_results.multi_face_landmarks):\n",
    "            #draw all landmarks\n",
    "            for i in range(0, len(face_landmarks.landmark)):      \n",
    "                landmark_x = face_landmarks.landmark[i].x * window_width\n",
    "                landmark_y = face_landmarks.landmark[i].y * window_height\n",
    "                \n",
    "                #add all landmarks in screen position to landmarks_all list\n",
    "                landmarks_all.append([int(landmark_x), int(landmark_y)])\n",
    "\n",
    "            #draw mouth landmarks\n",
    "            lips_list = np.zeros((len(mp_face_mesh.FACEMESH_LIPS), 2), np.int32)\n",
    "            \n",
    "            #empty list to hold points for lips\n",
    "            lipPoints = []\n",
    "            \n",
    "            #add to list of lip points based on lip landmarks\n",
    "            for x, y in mp_face_mesh.FACEMESH_LIPS:\n",
    "                duo = (x, y)\n",
    "                lipPoints.append(duo)\n",
    "                \n",
    "            #sort the lipPoints to be in order\n",
    "            sorted_lipPoints = sorted(lipPoints, key=lambda x: x[-1])\n",
    "            \n",
    "            #color for the chola inspired lipstick\n",
    "            lip_outline_color = (107, 32, 100);\n",
    "            \n",
    "            #get points to fill lips\n",
    "            lipPoints_polygon = findLipConnections();\n",
    "            lipPoints_toFill = [] \n",
    "            \n",
    "            #get landmark positions to fill lips\n",
    "            for lipPoint in lipPoints_polygon:\n",
    "                lipPoints_toFill.append(landmarks_all[lipPoint])\n",
    "            \n",
    "            #make into numpy array\n",
    "            lipPoints_toFill_np = np.array(lipPoints_toFill, dtype=np.int32)\n",
    "            \n",
    "            #go through each lipPoint to draw a line to connect them\n",
    "            for index, lipPoint in enumerate(sorted_lipPoints):\n",
    "                x_point = lipPoint[0]\n",
    "                y_point = lipPoint[1]\n",
    "                \n",
    "                #get each start and end point of the landmark in screen space\n",
    "                start_point = landmarks_all[x_point]\n",
    "                end_point = landmarks_all[y_point]\n",
    "                \n",
    "                #draw a line connecting all of the lip points, gives us a chola lip inspired look\n",
    "                frame = cv2.line(frame, start_point, end_point, lip_outline_color, 4)\n",
    "            \n",
    "            #color for the filled lipstick\n",
    "#             fill = (255, 255, 255);\n",
    "            \n",
    "            # simple fill in the lips like lipstick\n",
    "#             frame = cv2.fillPoly(frame, pts=np.int32([lipPoints_toFill_np]), color =(0,50,255, )) \n",
    "            \n",
    "            # eclectic fill of lips -> get mask of lips first\n",
    "#             mask = np.zeros(frame.shape[:2], dtype=\"uint8\")\n",
    "#             mask = cv2.fillPoly(mask, pts=np.int32([lipPoints_toFill_np]), color =(255,255,255)) \n",
    "#             #make copy of frame to just get the pixels of lips\n",
    "#             frame_copy = frame.copy()\n",
    "#             frame_copy = cv2.bitwise_and(frame_copy, frame, mask=mask)\n",
    "#             #make lips color change\n",
    "#             frame_copy = bgr2hsv(frame_copy)\n",
    "#             #output the masked frame over the video frame\n",
    "#             frame = cv2.bitwise_or(frame_copy, frame)\n",
    "            \n",
    "            \n",
    "            \n",
    "#             #this line below can be used to achieve the lip lined look with mediapipe's own function draw landmarks\n",
    "#             mp_drawing.draw_landmarks(image=frame, landmark_list=face_landmarks,\n",
    "#                                       connections=mp_face_mesh.FACEMESH_LIPS,\n",
    "#                                       landmark_drawing_spec=None, \n",
    "#                                       connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())\n",
    "    \n",
    "    # Display the frame.\n",
    "    cv2.imshow('Face Filter', frame)\n",
    "    \n",
    "    # Wait for 1ms. If a key is pressed, retreive the ASCII code of the key.\n",
    "    k = cv2.waitKey(1) & 0xFF    \n",
    "    \n",
    "    # Check if 'ESC' is pressed and break the loop.\n",
    "    if(k == 27):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture Object and close the windows.                  \n",
    "camera_video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#                 cv2.circle(frame, (int(landmark_x), int(landmark_y)), 5, (255, 0, 0), -1)\n",
    "\n",
    "\n",
    "#             i = 0;\n",
    "#             for index, connection in mp_face_mesh.FACEMESH_LIPS:\n",
    "#                 landmark_x = int(face_landmarks.landmark[index].x * window_width)\n",
    "#                 landmark_y = int(face_landmarks.landmark[index].y * window_height)\n",
    "#                 cv2.circle(frame, (landmark_x, landmark_y), 5, (255, 0, 255), -1)\n",
    "                \n",
    "#                 lips_list[i] = (landmark_x, landmark_y)                                \n",
    "#                 i+=1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
