{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import itertools\n",
    "import numpy as np\n",
    "from time import time\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectFacialLandmarks(image, face_mesh):\n",
    "    '''\n",
    "    This function performs facial landmarks detection on an image.\n",
    "    Args:\n",
    "        image:     The input image of person(s) whose facial landmarks needs to be detected.\n",
    "        face_mesh: The face landmarks detection function required to perform the landmarks detection.\n",
    "    Returns:\n",
    "        output_image: A copy of input image with face landmarks drawn.\n",
    "        results:      The output of the facial landmarks detection on the input image.\n",
    "    '''\n",
    "    \n",
    "    # Perform the facial landmarks detection on the image, after converting it into RGB format.\n",
    "    # TODO: check why we have to put ::-1\n",
    "    results = face_mesh.process(image[:,:,::-1])\n",
    "    \n",
    "    #---------- PRINT THE LANDMARKS ON THE IMAGE ----------\n",
    "    \n",
    "    # Create a copy of the input image to draw facial landmarks.\n",
    "    output_image = image[:,:,::-1].copy()\n",
    "    \n",
    "    # Check if facial landmarks in the image are found.\n",
    "    if results.multi_face_landmarks:\n",
    "\n",
    "        # Iterate over the found faces.\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "\n",
    "            # Draw the facial landmarks on the output image with the face mesh tesselation\n",
    "            # connections using default face mesh tesselation style.\n",
    "            mp_drawing.draw_landmarks(image=output_image, landmark_list=face_landmarks,\n",
    "                                      connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                                      landmark_drawing_spec=None, \n",
    "                                      connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "\n",
    "            # Draw the facial landmarks on the output image with the face mesh contours\n",
    "            # connections using default face mesh contours style.\n",
    "            mp_drawing.draw_landmarks(image=output_image, landmark_list=face_landmarks,\n",
    "                                      connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                                      landmark_drawing_spec=None, \n",
    "                                      connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())\n",
    "            \n",
    "    #------------------------------------------------------\n",
    "    \n",
    "        \n",
    "    # Return the output image in BGR format and results of facial landmarks detection.\n",
    "    # TODO: check why we have to put ::-1\n",
    "    return np.ascontiguousarray(output_image[:,:,::-1], dtype=np.uint8), results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSize(image, face_landmarks, INDEXES):\n",
    "    '''\n",
    "    This function calculates the height and width of a face part utilizing its landmarks.\n",
    "    Args:\n",
    "        image:          The image of person(s) whose face part size is to be calculated.\n",
    "        face_landmarks: The detected face landmarks of the person whose face part size is to \n",
    "                        be calculated.\n",
    "        INDEXES:        The indexes of the face part landmarks, whose size is to be calculated.\n",
    "    Returns:\n",
    "        width:     The calculated width of the face part of the face whose landmarks were passed.\n",
    "        height:    The calculated height of the face part of the face whose landmarks were passed.\n",
    "        landmarks: An array of landmarks of the face part whose size is calculated.\n",
    "    '''\n",
    "    \n",
    "    # Retrieve the height and width of the image.\n",
    "    image_height, image_width, _ = image.shape\n",
    "    \n",
    "    # Initialize a list to store the landmarks of the face part.\n",
    "    landmarks = []\n",
    "    \n",
    "    # Iterate over the indexes of the landmarks of the face part. \n",
    "    for INDEX in INDEXES:\n",
    "        \n",
    "        # Append the landmark into the list.\n",
    "        landmarks.append([int(face_landmarks.landmark[INDEX].x * image_width),\n",
    "                               int(face_landmarks.landmark[INDEX].y * image_height)])\n",
    "    \n",
    "    # Calculate the width and height of the face part.\n",
    "    # TODO: Transform it to take into account the shearing --> find the 3D boundinx box and project \n",
    "    # the closest face to the screen\n",
    "    _, _, width, height = cv2.boundingRect(np.array(landmarks))\n",
    "    \n",
    "\n",
    "    # Convert the list of landmarks of the face part into a numpy array.\n",
    "    landmarks = np.array(landmarks)\n",
    "    \n",
    "    # Return the calculated width height and the landmarks of the face part.\n",
    "    return width, height, landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isOpen(image, face_mesh_results, face_part, threshold=5):\n",
    "    '''\n",
    "    This function checks whether the eye or mouth of the person(s) is open, \n",
    "    utilizing its facial landmarks.\n",
    "    Args:\n",
    "        image:             The image of person(s) whose an eye or mouth is to be checked.\n",
    "        face_mesh_results: The output of the facial landmarks detection on the image.\n",
    "        face_part:         The name of the face part that is required to check: MOUTH, RIGHT EYE, LEFT EYE\n",
    "        threshold:         The threshold value used to check the isOpen condition.\n",
    "    Returns:\n",
    "        status:       A dictionary containing isOpen statuses of the face part of all the \n",
    "                      detected faces.  \n",
    "    '''\n",
    "    \n",
    "    # Retrieve the height and width of the image.\n",
    "    image_height, image_width, _ = image.shape\n",
    "\n",
    "    \n",
    "    # Create a dictionary to store the isOpen status of the face part of all the detected faces.\n",
    "    status={}\n",
    "    \n",
    "    # Check if the face part is mouth.\n",
    "    if face_part == 'MOUTH':\n",
    "        \n",
    "        # Get the indexes of the mouth.\n",
    "        INDEXES = mp_face_mesh.FACEMESH_LIPS\n",
    "        \n",
    "    # Check if the face part is left eye.    \n",
    "    elif face_part == 'LEFT EYE':\n",
    "        \n",
    "        # Get the indexes of the left eye.\n",
    "        INDEXES = mp_face_mesh.FACEMESH_LEFT_EYE\n",
    "    \n",
    "    # Check if the face part is right eye.    \n",
    "    elif face_part == 'RIGHT EYE':\n",
    "        \n",
    "        # Get the indexes of the right eye.\n",
    "        INDEXES = mp_face_mesh.FACEMESH_RIGHT_EYE \n",
    "           \n",
    "    # Otherwise return nothing.\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    # Convert the indexes of the landmarks of the face part into a list.\n",
    "    # TODO: Rewrite it in a more natural way OR undestand it + Is it necessaray to create it as a list?\n",
    "    INDEXES_LIST = set(list(itertools.chain(*INDEXES)))\n",
    "    \n",
    "    # Iterate over the found faces.\n",
    "    for face_no, face_landmarks in enumerate(face_mesh_results.multi_face_landmarks):\n",
    "        \n",
    "        # Get the height of the face part.\n",
    "        _, height, _ = getSize(image, face_landmarks, INDEXES_LIST)\n",
    "        \n",
    "        # Get the height of the whole face.\n",
    "        face_oval = set(list(itertools.chain(*mp_face_mesh.FACEMESH_FACE_OVAL)))\n",
    "        _, face_height, _ = getSize(image, face_landmarks, face_oval)\n",
    "        \n",
    "        # Check if the face part is open.\n",
    "        if (height/face_height)*100 > threshold:\n",
    "            \n",
    "            # Set status of the face part to open.\n",
    "            status[face_no] = 'OPEN'\n",
    "        \n",
    "        # Otherwise.\n",
    "        else:\n",
    "            # Set status of the face part to close.\n",
    "            status[face_no] = 'CLOSE'\n",
    "    \n",
    "    # Otherwise\n",
    "    else:\n",
    "        \n",
    "        # Return the output image and the isOpen statuses of the face part of each detected face.\n",
    "        return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkRotation(face_landmarks):\n",
    "    face_landmarks_l = []\n",
    "    for index in range(len(face_landmarks.landmark)):\n",
    "        face_landmarks_l.append([face_landmarks.landmark[index].x, \n",
    "                          face_landmarks.landmark[index].y, \n",
    "                          face_landmarks.landmark[index].z])\n",
    "    \n",
    "    o3d_landmarks_f = o3d.utility.Vector3dVector(np.array(face_landmarks_l))\n",
    "    o3d_bbox_f = o3d.geometry.OrientedBoundingBox.create_from_points(o3d_landmarks_f)\n",
    "    \n",
    "    R = o3d_bbox_f.R\n",
    "    points = np.asarray(o3d_bbox_f.get_box_points())\n",
    "    \n",
    "    # After projection the points stored with this indexes\n",
    "    #  2*****7\n",
    "    #  *     *\n",
    "    #  *     *\n",
    "    #  0*****1\n",
    "    # So we check how they turned in local space to get the rotation/inversion\n",
    "    if points[2, 0] > points[1, 0] and points[2, 1] > points[1, 1]:\n",
    "        return 0, R, points\n",
    "    elif points[2, 0] < points[1, 0] and points[2, 1] < points[1, 1]:\n",
    "        return 1, R, points\n",
    "    elif points[2, 0] < points[1, 0] and points[2, 1] > points[1, 1]:\n",
    "        return 2, R, points\n",
    "    elif points[2, 0] > points[1, 0] and points[2, 1] < points[1, 1]:\n",
    "        return 3, R, points\n",
    "    else:\n",
    "        return 4, R, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundingBox(face_landmarks, face_part, INDEXES, angle, R, required_size):\n",
    "    \n",
    "    landmarks = []\n",
    "    for index in INDEXES:\n",
    "        landmarks.append([face_landmarks.landmark[index].x, \n",
    "                          face_landmarks.landmark[index].y, \n",
    "                          face_landmarks.landmark[index].z])\n",
    "    \n",
    "    # Create the oriented bounding box\n",
    "    o3d_landmarks = o3d.utility.Vector3dVector(np.array(landmarks))\n",
    "    o3d_bbox = o3d.geometry.OrientedBoundingBox.create_from_points(o3d_landmarks)\n",
    "    \n",
    "    # Get box points and center point\n",
    "    box_points = np.asarray(o3d_bbox.get_box_points())\n",
    "    box_center = np.asarray(o3d_bbox.get_center())\n",
    "\n",
    "    \n",
    "    # Project in global space and center \n",
    "    R_inv = np.linalg.inv(R)\n",
    "    \n",
    "    \n",
    "    # One of the forward point\n",
    "    # We only need to recover the global depth to project the filter image on the forward rectangle\n",
    "    #print(box_points[:,2])\n",
    "    index = np.argmin(box_points[:,2])\n",
    "    forward_point = box_points[index, :] \n",
    "    forward_point_projected = np.matmul(R_inv, forward_point - box_center)\n",
    "    \n",
    "\n",
    "    \n",
    "    # Apply transformations (depends on the filter type)\n",
    "    required_width, required_height = required_size\n",
    "    forward_points_transformed = []\n",
    "\n",
    "    # Deal with the position depending on the filter type AND the orientation of the face bounding box\n",
    "    if face_part == 'FOREHEAD':        \n",
    "        if angle == 0:            \n",
    "            forward_points_transformed.append([required_height, -required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([0, -required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([0, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([required_height, required_width / 2, forward_point_projected[2]])\n",
    "        elif angle == 1:\n",
    "            forward_points_transformed.append([-required_height, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([0, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([0, - required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([-required_height, - required_width / 2, forward_point_projected[2]])\n",
    "        elif angle == 2:\n",
    "            forward_points_transformed.append([required_height, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([0, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([0, - required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([required_height, - required_width / 2, forward_point_projected[2]])\n",
    "        elif angle == 3:\n",
    "            forward_points_transformed.append([-required_height, -required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([0, -required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([0, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([-required_height, required_width / 2, forward_point_projected[2]])\n",
    "        else:\n",
    "            return\n",
    "\n",
    "            \n",
    "    elif face_part == 'RIGHT EYE' or face_part == 'LEFT EYE':  \n",
    "        if angle == 0:            \n",
    "            forward_points_transformed.append([required_height/2, -required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([-required_height/2, -required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([-required_height/2, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([required_height/2, required_width / 2, forward_point_projected[2]])\n",
    "        elif angle == 1:\n",
    "            forward_points_transformed.append([-required_height/2, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([required_height/2, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([required_height/2, - required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([-required_height/2, - required_width / 2, forward_point_projected[2]])\n",
    "        elif angle == 2:\n",
    "            forward_points_transformed.append([required_height/2, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([-required_height/2, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([-required_height/2, - required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([required_height/2, - required_width / 2, forward_point_projected[2]])\n",
    "        elif angle == 3:\n",
    "            forward_points_transformed.append([-required_height/2, -required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([required_height/2, -required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([required_height/2, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([-required_height/2, required_width / 2, forward_point_projected[2]])\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "    elif face_part == 'MOUTH':\n",
    "        if angle == 0:            \n",
    "            forward_points_transformed.append([0, -required_width / 2, 0])\n",
    "            forward_points_transformed.append([-required_height, -required_width / 2, 0])\n",
    "            forward_points_transformed.append([-required_height, required_width / 2, 0])\n",
    "            forward_points_transformed.append([0, required_width / 2, 0])\n",
    "        elif angle == 1:\n",
    "            forward_points_transformed.append([0, required_width / 2, 0])\n",
    "            forward_points_transformed.append([required_height, required_width / 2, 0])\n",
    "            forward_points_transformed.append([required_height, - required_width / 2, 0])\n",
    "            forward_points_transformed.append([0, - required_width / 2, 0])\n",
    "        elif angle == 2:\n",
    "            forward_points_transformed.append([0, required_width / 2, 0])\n",
    "            forward_points_transformed.append([-required_height, required_width / 2, 0])\n",
    "            forward_points_transformed.append([-required_height, - required_width / 2, 0])\n",
    "            forward_points_transformed.append([0, - required_width / 2, 0])\n",
    "        elif angle == 3:\n",
    "            forward_points_transformed.append([0, -required_width / 2, 0])\n",
    "            forward_points_transformed.append([required_height, -required_width / 2, 0])\n",
    "            forward_points_transformed.append([required_height, required_width / 2, 0])\n",
    "            forward_points_transformed.append([0, required_width / 2, 0])\n",
    "        else:\n",
    "            return     \n",
    "    else:\n",
    "        return\n",
    "       \n",
    "    # Project the filter image in the local space\n",
    "    forward_points = []\n",
    "    for p in forward_points_transformed:\n",
    "        p_rotated = np.matmul(R, p) # Rotate to local space\n",
    "        p_translated = p_rotated + box_center # Translate to box position\n",
    "        forward_points.append(p_translated)\n",
    "            \n",
    "    return o3d_bbox, box_center, forward_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawFilter(image, filter_img, face_landmarks, face_part, INDEXES, angle, R, required_size):\n",
    "    \n",
    "    filter_img_height, filter_img_width, _  = filter_img.shape\n",
    "    image_height, image_width, _ = image.shape\n",
    "    \n",
    "    required_width = required_size[0]\n",
    "    required_height = required_size[1]\n",
    "    \n",
    "    # Get 3d bounding box and fit the filter image in it\n",
    "    required_height_norm = required_height / image_height\n",
    "    required_width_norm = required_width / image_width\n",
    "    \n",
    "    # Get the bounding box of the right size\n",
    "    o3d_bbox, box_center_array, forward_points = boundingBox(face_landmarks, face_part, INDEXES, angle, R,\n",
    "                                                             [required_width_norm, required_height_norm])\n",
    "    #print(forward_points)\n",
    "    \n",
    "    #o3d.visualization.draw_geometries([o3d_bbox])\n",
    "          \n",
    "    # Get the initial corner points of the filter image (counterclockwise)\n",
    "    pts1 = np.array([[0, 0], [0, filter_img_height], \n",
    "                     [filter_img_width, filter_img_height], [filter_img_width, 0]])\n",
    "    \n",
    "    # Get the final corner points of the filter image in the overlayered image (counterclockwise)\n",
    "    # TODO: mettre en perspective au lieu de orthographic\n",
    "    pts2 = []\n",
    "    for point in forward_points:\n",
    "        resolution_x = 1\n",
    "        resolution_y = 1\n",
    "\n",
    "        fov = 45\n",
    "        z0 = (resolution_x / 2) / np.tan((fov/2) * np.pi / 180)\n",
    "\n",
    "        \n",
    "        relative_x = int(((point[0] - 0.5) * z0 / (z0 + point[2]) + 0.5) * image.shape[1])\n",
    "        relative_y = int(((point[1] - 0.5) * z0 / (z0 + point[2]) + 0.5) * image.shape[0])\n",
    "\n",
    "\n",
    "        pts2.append([relative_x, relative_y])\n",
    "    pts2 = np.array(pts2)\n",
    "\n",
    "    # Find homography from the filter image to the overlayed image\n",
    "    h, mask = cv2.findHomography(pts1, pts2, cv2.RANSAC,5.0)\n",
    "\n",
    "    # Put the filter image in perspective\n",
    "    im1Reg = cv2.warpPerspective(filter_img, h, (image_width, image_height))\n",
    "\n",
    "    # Inverse filter image mask in perspective\n",
    "    _, filter_img_mask = cv2.threshold(cv2.cvtColor(im1Reg, cv2.COLOR_BGR2GRAY),\n",
    "                                   25, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    # Spread in 3 dimensions (rgb)\n",
    "    filter_img_mask = np.expand_dims(filter_img_mask, axis=2)\n",
    "    filter_img_mask = np.repeat(filter_img_mask, 3, axis=2)\n",
    "\n",
    "    # Final image with black pixels at filter image position\n",
    "    resultant_image = cv2.bitwise_and(image, filter_img_mask)\n",
    "\n",
    "    # Use Bitwise or to merge the two images\n",
    "    annotated_image = cv2.bitwise_or(im1Reg, resultant_image)        \n",
    "\n",
    "    # DEBUG\n",
    "    box_points = np.asarray(o3d_bbox.get_box_points())\n",
    "\n",
    "    R = o3d_bbox.R\n",
    "    R_inv = np.linalg.inv(R)\n",
    "    \n",
    "    points_projected = []\n",
    "    for p in box_points:\n",
    "        points_projected.append(np.matmul(R_inv, p - box_center_array))\n",
    "    points_projected = np.array(points_projected)\n",
    "    #print(points_projected)\n",
    "    \n",
    "    relative_x = int((box_center_array[0] * z0 / (z0 + box_center_array[2])) * frame.shape[1])\n",
    "    relative_y = int((box_center_array[1] * z0 / (z0 + box_center_array[2])) * frame.shape[0])\n",
    "\n",
    "    #cv2.circle(annotated_image, (relative_x, relative_y), radius=1, color=(0, 0, 255), thickness=7)\n",
    "    \n",
    "    i = 0\n",
    "    #for point in box_points[[3, 4, 5, 6], :]:\n",
    "    for point in box_points[[0, 1, 2, 7], :]:\n",
    "        relative_x = int((point[0] * z0 / (z0 + point[2])) * frame.shape[1])\n",
    "        relative_y = int((point[1] * z0 / (z0 + point[2])) * frame.shape[0])\n",
    "\n",
    "        #if point[2] < 0:\n",
    "        #    color = (255, 0, 0) # négatif c'est devant !\n",
    "        #else:\n",
    "        #    color = (0, 255, 0)\n",
    "        #cv2.circle(annotated_image, (relative_x, relative_y), radius=1, color=(0, 0+i, 0), thickness=7)\n",
    "        i+= 70\n",
    "\n",
    "    return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the mediapipe face mesh class.\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "# Setup the face landmarks function for videos.\n",
    "# TODO: Here works for only one face --> Make it work for multiple faces (see overlay call in main function)\n",
    "face_mesh_videos = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, \n",
    "                                         min_detection_confidence=0.5,min_tracking_confidence=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the mediapipe drawing class.\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize the mediapipe drawing styles class.\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the VideoCapture object to read from the webcam.\n",
    "camera_video = cv2.VideoCapture(0)\n",
    "\n",
    "# Set camera resolution\n",
    "camera_video.set(3,1280)\n",
    "camera_video.set(4,960)\n",
    "\n",
    "# Create named window for resizing purposes.\n",
    "cv2.namedWindow('Face Filter', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Read the left and right eyes images.\n",
    "# TODO: Create an interactive window to let the user select the wanted filters\n",
    "left_eye = cv2.imread('data/left_eye_cupcake.png')\n",
    "right_eye = cv2.imread('data/right_eye_cupcake.png')\n",
    "\n",
    "crown = cv2.imread('data/crown.png')\n",
    "\n",
    "# Initialize the VideoCapture object to read from the smoke animation video stored in the disk.\n",
    "# TODO: Same as for images --> Create a dictionary to store all images and videos?\n",
    "animation = cv2.VideoCapture('data/rainbow_animation1.mp4')\n",
    "\n",
    "# Set the smoke animation video frame counter to zero.\n",
    "animation_frame_counter = 0\n",
    "\n",
    "# Iterate until the webcam is accessed successfully.\n",
    "while camera_video.isOpened():\n",
    "    \n",
    "    # Read a frame.\n",
    "    ok, frame = camera_video.read()\n",
    "    \n",
    "    # Check if frame is not read properly then continue to the next iteration to read\n",
    "    # the next frame.\n",
    "    if not ok:\n",
    "        continue\n",
    "        \n",
    "    # Get the width and height of the image\n",
    "    image_height, image_width, _ = frame.shape\n",
    "        \n",
    "    # Read a frame from smoke animation video\n",
    "    _, animation_frame = animation.read()\n",
    "    \n",
    "    # Increment the smoke animation video frame counter.\n",
    "    animation_frame_counter += 1\n",
    "    \n",
    "    # Check if the current frame is the last frame of the animation video.\n",
    "    if animation_frame_counter == animation.get(cv2.CAP_PROP_FRAME_COUNT):     \n",
    "        \n",
    "        # Set the current frame position to first frame to restart the video.\n",
    "        animation.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "        \n",
    "        # Set the animation video frame counter to zero.\n",
    "        animation_frame_counter = 0\n",
    "    \n",
    "    # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Perform Face landmarks detection.\n",
    "    _, face_mesh_results = detectFacialLandmarks(frame, face_mesh_videos)\n",
    "    \n",
    "    \n",
    "    # Check if facial landmarks are found.\n",
    "    if face_mesh_results.multi_face_landmarks:        \n",
    "        # Get the mouth isOpen status of the person in the frame.\n",
    "        mouth_status = isOpen(frame, face_mesh_results, 'MOUTH', \n",
    "                                     threshold=15)\n",
    "        \n",
    "        # Get the left eye isOpen status of the person in the frame.\n",
    "        left_eye_status = isOpen(frame, face_mesh_results, 'LEFT EYE', \n",
    "                                        threshold=4.5)\n",
    "        \n",
    "        # Get the right eye isOpen status of the person in the frame.\n",
    "        right_eye_status = isOpen(frame, face_mesh_results, 'RIGHT EYE', \n",
    "                                         threshold=4.5)\n",
    "        \n",
    "        # Iterate over the found faces.\n",
    "        for face_num, face_landmarks in enumerate(face_mesh_results.multi_face_landmarks):\n",
    "            \n",
    "            angle, R, points = checkRotation(face_landmarks)\n",
    "            \n",
    "            # Check if the left eye of the face is open.\n",
    "            if left_eye_status[face_num] == 'OPEN':\n",
    "                \n",
    "                # Get the width and height of filter image.\n",
    "                filter_img_height, filter_img_width, _  = left_eye.shape\n",
    "                \n",
    "                # Convert the indexes of the landmarks of the face part into a list.\n",
    "                # TODO: Rewrite it in a more natural way OR undestand it + Is it necessaray to create it as a list?\n",
    "                left_eye_landmarks = set(list(itertools.chain(*mp_face_mesh.FACEMESH_LEFT_EYE)))\n",
    "                \n",
    "                            # Get the height of the face part on which we will overlay the filter image.\n",
    "                _, face_part_height, landmarks = getSize(frame, face_landmarks, left_eye_landmarks)\n",
    "\n",
    "                # Specify the height to which the filter image is required to be resized.\n",
    "                # 2.5 can be changed depending on the size of the filter we want\n",
    "                # This allows the filter to be bigger/smaller depending on both the size of the eye and the size of the aperture\n",
    "                # of the eye\n",
    "                required_height = int(face_part_height*2.5)\n",
    "                required_width = int(filter_img_width*(required_height/filter_img_height))\n",
    "                \n",
    "                frame = drawFilter(frame, left_eye, face_landmarks, 'LEFT EYE',\n",
    "                                   left_eye_landmarks, angle, R, [required_width, required_height])\n",
    "                \n",
    "            \n",
    "            # Check if the right eye of the face is open.\n",
    "            if right_eye_status[face_num] == 'OPEN':\n",
    "                \n",
    "                # Get the width and height of filter image.\n",
    "                filter_img_height, filter_img_width, _  = right_eye.shape\n",
    "                \n",
    "                # Convert the indexes of the landmarks of the face part into a list.\n",
    "                # TODO: Rewrite it in a more natural way OR undestand it + Is it necessaray to create it as a list?\n",
    "                right_eye_landmarks = set(list(itertools.chain(*mp_face_mesh.FACEMESH_RIGHT_EYE)))\n",
    "                \n",
    "                # Get the height of the face part on which we will overlay the filter image.\n",
    "                _, face_part_height, landmarks = getSize(frame, face_landmarks, right_eye_landmarks)\n",
    "\n",
    "                # Specify the height to which the filter image is required to be resized.\n",
    "                # 2.5 can be changed depending on the size of the filter we want\n",
    "                # This allows the filter to be bigger/smaller depending on both the size of the eye and the size of the aperture\n",
    "                # of the eye\n",
    "                required_height = int(face_part_height*2.5)\n",
    "                required_width = int(filter_img_width*(required_height/filter_img_height))\n",
    "                \n",
    "                frame = drawFilter(frame, right_eye, face_landmarks, 'RIGHT EYE',\n",
    "                                   right_eye_landmarks, angle, R, [required_width, required_height])\n",
    "                            \n",
    "            # Check if the mouth of the face is open.\n",
    "            if mouth_status[face_num] == 'OPEN':\n",
    "                \n",
    "                # Get the width and height of filter image.\n",
    "                filter_img_height, filter_img_width, _  = animation_frame.shape\n",
    "                \n",
    "                # Convert the indexes of the landmarks of the face part into a list.\n",
    "                # TODO: Rewrite it in a more natural way OR undestand it + Is it necessaray to create it as a list?\n",
    "                lips_landmarks = set(list(itertools.chain(*mp_face_mesh.FACEMESH_LIPS)))\n",
    "                \n",
    "                # Get the height of the face part on which we will overlay the filter image.\n",
    "                face_part_width, face_part_height, landmarks = getSize(frame, face_landmarks, lips_landmarks)\n",
    "                \n",
    "                # Specify the height to which the filter image is required to be resized.\n",
    "                # 2.5 can be changed depending on the size of the filter we want\n",
    "                # This allows the filter to be bigger/smaller depending on both the size of the eye and the size of the aperture\n",
    "                # of the eye\n",
    "                #required_height = int(face_part_height*1.5)\n",
    "                #required_width = int(filter_img_width*(required_height/filter_img_height))\n",
    "                required_width = int(face_part_width*0.7)\n",
    "                required_height = int(filter_img_height*(required_width/filter_img_width))\n",
    "                \n",
    "                frame = drawFilter(frame, animation_frame, face_landmarks, 'MOUTH', \n",
    "                                   lips_landmarks, angle, R, [required_width, required_height])\n",
    "            \n",
    "            # FOREHEAD\n",
    "            # Get the width and height of filter image.\n",
    "            filter_img_height, filter_img_width, _  = crown.shape\n",
    "            \n",
    "            forehead_landmarks = [103, 67, 109, 10, 338, 297, 332]\n",
    "            \n",
    "            # Get the height of the face part on which we will overlay the filter image.\n",
    "            face_part_width,_, landmarks = getSize(frame, face_landmarks, forehead_landmarks)\n",
    "                \n",
    "            # Specify the height to which the filter image is required to be resized.\n",
    "            # 2.5 can be changed depending on the size of the filter we want\n",
    "            # This allows the filter to be bigger/smaller depending on both the size of the eye and the size of the aperture\n",
    "            # of the eye\n",
    "            required_width = int(face_part_width*2.5)\n",
    "            required_height = int(filter_img_height*(required_width/filter_img_width))\n",
    "            \n",
    "            frame = drawFilter(frame, crown, face_landmarks, 'FOREHEAD',\n",
    "                                   forehead_landmarks, angle, R, [required_width, required_height])\n",
    "            \n",
    "            i = 0\n",
    "            #for point in box_points[[3, 4, 5, 6], :]:\n",
    "            for point in points[[0, 1, 2, 7], :]:\n",
    "                resolution_x = 1\n",
    "                resolution_y = 1\n",
    "                fov = 45\n",
    "                z0 = (resolution_x / 2) / np.tan((fov/2) * np.pi / 180)\n",
    "        \n",
    "                relative_x = int((point[0] * z0 / (z0 + point[2]))* frame.shape[1])\n",
    "                relative_y = int((point[1] * z0 / (z0 + point[2])) * frame.shape[0])\n",
    "\n",
    "                #if point[2] < 0:\n",
    "                #    color = (255, 0, 0) # négatif c'est devant !\n",
    "                #else:\n",
    "                #    color = (0, 255, 0)\n",
    "                #cv2.circle(frame, (relative_x, relative_y), radius=1, color=(0, 0+i, 0), thickness=7)\n",
    "                i+= 70\n",
    "        \n",
    "    \n",
    "    # Display the frame.\n",
    "    cv2.imshow('Face Filter', frame)\n",
    "    \n",
    "    # Wait for 1ms. If a key is pressed, retreive the ASCII code of the key.\n",
    "    k = cv2.waitKey(1) & 0xFF    \n",
    "    \n",
    "    # Check if 'ESC' is pressed and break the loop.\n",
    "    if(k == 27):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture Object and close the windows.                  \n",
    "camera_video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF573",
   "language": "python",
   "name": "inf573"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
