{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import itertools\n",
    "import numpy as np\n",
    "from time import time\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectFacialLandmarks(image, face_mesh):\n",
    "    '''\n",
    "    This function performs facial landmarks detection on an image.\n",
    "    Args:\n",
    "        image:     The input image of person(s) whose facial landmarks needs to be detected.\n",
    "        face_mesh: The face landmarks detection function required to perform the landmarks detection.\n",
    "    Returns:\n",
    "        output_image: A copy of input image with face landmarks drawn.\n",
    "        results:      The output of the facial landmarks detection on the input image.\n",
    "    '''\n",
    "    \n",
    "    # Perform the facial landmarks detection on the image, after converting it into RGB format.\n",
    "    # TODO: check why we have to put ::-1\n",
    "    results = face_mesh.process(image[:,:,::-1])\n",
    "    \n",
    "    #---------- PRINT THE LANDMARKS ON THE IMAGE ----------\n",
    "    \n",
    "    # Create a copy of the input image to draw facial landmarks.\n",
    "    output_image = image[:,:,::-1].copy()\n",
    "    \n",
    "    # Check if facial landmarks in the image are found.\n",
    "    if results.multi_face_landmarks:\n",
    "\n",
    "        # Iterate over the found faces.\n",
    "        for face_landmarks in results.multi_face_landmarks:\n",
    "\n",
    "            # Draw the facial landmarks on the output image with the face mesh tesselation\n",
    "            # connections using default face mesh tesselation style.\n",
    "            mp_drawing.draw_landmarks(image=output_image, landmark_list=face_landmarks,\n",
    "                                      connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                                      landmark_drawing_spec=None, \n",
    "                                      connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "\n",
    "            # Draw the facial landmarks on the output image with the face mesh contours\n",
    "            # connections using default face mesh contours style.\n",
    "            mp_drawing.draw_landmarks(image=output_image, landmark_list=face_landmarks,\n",
    "                                      connections=mp_face_mesh.FACEMESH_CONTOURS,\n",
    "                                      landmark_drawing_spec=None, \n",
    "                                      connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_contours_style())\n",
    "            \n",
    "    #------------------------------------------------------\n",
    "    \n",
    "        \n",
    "    # Return the output image in BGR format and results of facial landmarks detection.\n",
    "    # TODO: check why we have to put ::-1\n",
    "    return np.ascontiguousarray(output_image[:,:,::-1], dtype=np.uint8), results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSize(image, face_landmarks, INDEXES):\n",
    "    '''\n",
    "    This function calculates the height and width of a face part utilizing its landmarks.\n",
    "    Args:\n",
    "        image:          The image of person(s) whose face part size is to be calculated.\n",
    "        face_landmarks: The detected face landmarks of the person whose face part size is to \n",
    "                        be calculated.\n",
    "        INDEXES:        The indexes of the face part landmarks, whose size is to be calculated.\n",
    "    Returns:\n",
    "        width:     The calculated width of the face part of the face whose landmarks were passed.\n",
    "        height:    The calculated height of the face part of the face whose landmarks were passed.\n",
    "        landmarks: An array of landmarks of the face part whose size is calculated.\n",
    "    '''\n",
    "    \n",
    "    # Retrieve the height and width of the image.\n",
    "    image_height, image_width, _ = image.shape\n",
    "    \n",
    "    # Initialize a list to store the landmarks of the face part.\n",
    "    landmarks = []\n",
    "    \n",
    "    # Iterate over the indexes of the landmarks of the face part. \n",
    "    for INDEX in INDEXES:\n",
    "        \n",
    "        # Append the landmark into the list.\n",
    "        landmarks.append([int(face_landmarks.landmark[INDEX].x * image_width),\n",
    "                               int(face_landmarks.landmark[INDEX].y * image_height)])\n",
    "    \n",
    "    # Calculate the width and height of the face part.\n",
    "    # TODO: Transform it to take into account the shearing --> find the 3D boundinx box and project \n",
    "    # the closest face to the screen\n",
    "    _, _, width, height = cv2.boundingRect(np.array(landmarks))\n",
    "    \n",
    "\n",
    "    # Convert the list of landmarks of the face part into a numpy array.\n",
    "    landmarks = np.array(landmarks)\n",
    "    \n",
    "    # Return the calculated width height and the landmarks of the face part.\n",
    "    return width, height, landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isOpen(image, face_mesh_results, face_part, threshold=5):\n",
    "    '''\n",
    "    This function checks whether the eye or mouth of the person(s) is open, \n",
    "    utilizing its facial landmarks.\n",
    "    Args:\n",
    "        image:             The image of person(s) whose an eye or mouth is to be checked.\n",
    "        face_mesh_results: The output of the facial landmarks detection on the image.\n",
    "        face_part:         The name of the face part that is required to check: MOUTH, RIGHT EYE, LEFT EYE\n",
    "        threshold:         The threshold value used to check the isOpen condition.\n",
    "    Returns:\n",
    "        status:       A dictionary containing isOpen statuses of the face part of all the \n",
    "                      detected faces.  \n",
    "    '''\n",
    "    \n",
    "    # Retrieve the height and width of the image.\n",
    "    image_height, image_width, _ = image.shape\n",
    "\n",
    "    \n",
    "    # Create a dictionary to store the isOpen status of the face part of all the detected faces.\n",
    "    status={}\n",
    "    \n",
    "    # Check if the face part is mouth.\n",
    "    if face_part == 'MOUTH':\n",
    "        \n",
    "        # Get the indexes of the mouth.\n",
    "        INDEXES = mp_face_mesh.FACEMESH_LIPS\n",
    "        \n",
    "    # Check if the face part is left eye.    \n",
    "    elif face_part == 'LEFT EYE':\n",
    "        \n",
    "        # Get the indexes of the left eye.\n",
    "        INDEXES = mp_face_mesh.FACEMESH_LEFT_EYE\n",
    "    \n",
    "    # Check if the face part is right eye.    \n",
    "    elif face_part == 'RIGHT EYE':\n",
    "        \n",
    "        # Get the indexes of the right eye.\n",
    "        INDEXES = mp_face_mesh.FACEMESH_RIGHT_EYE \n",
    "           \n",
    "    # Otherwise return nothing.\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    # Convert the indexes of the landmarks of the face part into a list.\n",
    "    # TODO: Rewrite it in a more natural way OR undestand it + Is it necessaray to create it as a list?\n",
    "    INDEXES_LIST = set(list(itertools.chain(*INDEXES)))\n",
    "    \n",
    "    # Iterate over the found faces.\n",
    "    for face_no, face_landmarks in enumerate(face_mesh_results.multi_face_landmarks):\n",
    "        \n",
    "        # Get the height of the face part.\n",
    "        _, height, _ = getSize(image, face_landmarks, INDEXES_LIST)\n",
    "        \n",
    "        # Get the height of the whole face.\n",
    "        face_oval = set(list(itertools.chain(*mp_face_mesh.FACEMESH_FACE_OVAL)))\n",
    "        _, face_height, _ = getSize(image, face_landmarks, face_oval)\n",
    "        \n",
    "        # Check if the face part is open.\n",
    "        if (height/face_height)*100 > threshold:\n",
    "            \n",
    "            # Set status of the face part to open.\n",
    "            status[face_no] = 'OPEN'\n",
    "        \n",
    "        # Otherwise.\n",
    "        else:\n",
    "            # Set status of the face part to close.\n",
    "            status[face_no] = 'CLOSE'\n",
    "    \n",
    "    # Otherwise\n",
    "    else:\n",
    "        \n",
    "        # Return the output image and the isOpen statuses of the face part of each detected face.\n",
    "        return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundingBox(face_landmarks, INDEXES, required_size):\n",
    "    \n",
    "    landmarks = []\n",
    "    for index in INDEXES:\n",
    "        landmarks.append([face_landmarks.landmark[index].x, \n",
    "                          face_landmarks.landmark[index].y, \n",
    "                          face_landmarks.landmark[index].z])\n",
    "    \n",
    "    # Create the oriented bounding box\n",
    "    o3d_landmarks = o3d.utility.Vector3dVector(np.array(landmarks))\n",
    "    o3d_bbox = o3d.geometry.OrientedBoundingBox.create_from_points(o3d_landmarks)\n",
    "    \n",
    "\n",
    "    #width, height, depth = o3d_bbox.get_axis_aligned_bounding_box().get_extent()\n",
    "    #print(\"(\" + str(width) + \",\" + str(height) + \")\")\n",
    "    \n",
    "    # Get box points and center point\n",
    "    box_points = np.asarray(o3d_bbox.get_box_points())\n",
    "    box_center = np.asarray(o3d_bbox.get_center())\n",
    "    \n",
    "    # TODO : Improve the retrieval of points + do I have to put them in the right order here?\n",
    "    \n",
    "    # Retrieve forward points on the left\n",
    "    #left_points = []\n",
    "    #for p in box_points:\n",
    "    #    if p[0] < box_center[0]:\n",
    "    #        left_points.append(list(p))\n",
    "    #left_points = sorted(left_points, key=lambda l: l[2])[:2]\n",
    "    #left_points = sorted(left_points, key=lambda l: l[1])\n",
    "    #print(left_points)\n",
    "    \n",
    "    # Retrieve forward points on the right\n",
    "    #right_points = []\n",
    "    #for p in box_points:\n",
    "    #    if p[0] > box_center[0]:\n",
    "    #        right_points.append(list(p))\n",
    "    #right_points = sorted(right_points, key=lambda l: l[2])[:2]\n",
    "    #right_points = sorted(right_points, key=lambda l: l[1], reverse=True)\n",
    "    \n",
    "    # Combine points --> Retrieve the forward rectangle of the bounding box\n",
    "    #forward_points = np.array(left_points + right_points)\n",
    "    #print(forward_points)\n",
    "    \n",
    "    #forward_points = box_points[[2, 0, 1, 7], :]\n",
    "    #print(forward_points)\n",
    "    #print(forward_points)\n",
    "    \n",
    "    # Project in global space and center \n",
    "    # TODO: maybe put them back in the right order here ???\n",
    "    R = o3d_bbox.R\n",
    "    R_inv = np.linalg.inv(R)\n",
    "    \n",
    "    forward_point = box_points[2, :]\n",
    "    forward_point_projected = np.matmul(R_inv, forward_point - box_center)\n",
    "    \n",
    "    #forward_points_projected = []\n",
    "    #for p in forward_points:\n",
    "    #    p_translated = p - box_center # Put the origin of the local space at the origin of the global space\n",
    "    #    p_rotated = np.matmul(R_inv, p_translated) # Rotate to be in the global space\n",
    "    #    forward_points_projected.append(p_rotated)\n",
    "    #forward_points_projected = np.array(forward_points_projected)\n",
    "    #print(forward_points_projected)\n",
    "    \n",
    "    # Apply transformations (depends on the filter type)\n",
    "    required_width, required_height = required_size\n",
    "    forward_points_transformed = []\n",
    "    #forward_points_transformed.append([- required_width / 2, required_height / 2, forward_points_projected[0, 2]])\n",
    "    #forward_points_transformed.append([- required_width / 2, - required_height / 2, forward_points_projected[1, 2]])\n",
    "    #forward_points_transformed.append([required_width / 2, - required_height / 2, forward_points_projected[2, 2]])\n",
    "    #forward_points_transformed.append([required_width / 2, required_height / 2, forward_points_projected[3, 2]])\n",
    "    \n",
    "    # Je crois que pour la bouche ça tourne de 90° et ca fait de la merde du coup\n",
    "    forward_points_transformed.append([- required_width / 2, required_height / 2, forward_point_projected[2]])\n",
    "    forward_points_transformed.append([- required_width / 2, - required_height / 2, forward_point_projected[2]])\n",
    "    forward_points_transformed.append([required_width / 2, - required_height / 2, forward_point_projected[2]])\n",
    "    forward_points_transformed.append([required_width / 2, required_height / 2, forward_point_projected[2]])\n",
    "    \n",
    "    forward_points = []\n",
    "    for p in forward_points_transformed:\n",
    "        p_rotated = np.matmul(R, p) # Rotate back\n",
    "        p_translated = p_rotated + box_center # Translate back\n",
    "        forward_points.append(p_translated)\n",
    "    forward_points = sorted(forward_points, key=lambda l: l[0])\n",
    "    forward_points[2:] = sorted(forward_points[2:], key=lambda l: l[1], reverse = True)\n",
    "    forward_points[:2] = sorted(forward_points[:2], key=lambda l: l[1])\n",
    "    forward_points = np.array(forward_points)\n",
    "    # TODO : remettre les points dans le bon ordre\n",
    "    \n",
    "    \n",
    "    return o3d_bbox, box_center, forward_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawFilter(image, filter_img, face_landmarks, INDEXES, required_size):\n",
    "    \n",
    "    filter_img_height, filter_img_width, _  = filter_img.shape\n",
    "    image_height, image_width, _ = image.shape\n",
    "    \n",
    "    required_width = required_size[0]\n",
    "    required_height = required_size[1]\n",
    "    \n",
    "    # Get 3d bounding box and fit the filter image in it\n",
    "    required_height_norm = required_height / image_height\n",
    "    required_width_norm = required_width / image_width\n",
    "    o3d_bbox, box_center_array, forward_points = boundingBox(face_landmarks, INDEXES, \n",
    "                                                             [required_width_norm, required_height_norm])\n",
    "\n",
    "    #print(forward_points)\n",
    "\n",
    "    pts1 = np.array([[0, 0], [0, filter_img_height], \n",
    "                     [filter_img_width, filter_img_height], [filter_img_width, 0]])\n",
    "    pts2 = []\n",
    "    for point in forward_points:\n",
    "        relative_x = int(point[0] * image.shape[1])\n",
    "        relative_y = int(point[1] * image.shape[0])\n",
    "\n",
    "        pts2.append([relative_x, relative_y])\n",
    "    pts2 = np.array(pts2)\n",
    "\n",
    "    h, mask = cv2.findHomography(pts1, pts2, cv2.RANSAC,5.0)\n",
    "\n",
    "    im1Reg = cv2.warpPerspective(filter_img, h, (image_width, image_height)) # Filter image in perspective\n",
    "\n",
    "    _, filter_img_mask = cv2.threshold(cv2.cvtColor(im1Reg, cv2.COLOR_BGR2GRAY),\n",
    "                                   25, 255, cv2.THRESH_BINARY_INV) # Inverse filter image mask\n",
    "\n",
    "    filter_img_mask = np.expand_dims(filter_img_mask, axis=2)\n",
    "    filter_img_mask = np.repeat(filter_img_mask, 3, axis=2)\n",
    "\n",
    "    resultant_image = cv2.bitwise_and(image, filter_img_mask) # Image with black pixel at filter image position\n",
    "\n",
    "    #Using Bitwise or to merge the two images\n",
    "    annotated_image = cv2.bitwise_or(im1Reg, resultant_image)        \n",
    "\n",
    "    relative_x = int(box_center_array[0] * frame.shape[1])\n",
    "    relative_y = int(box_center_array[1] * frame.shape[0])\n",
    "\n",
    "    cv2.circle(annotated_image, (relative_x, relative_y), radius=1, color=(0, 0, 255), thickness=7)\n",
    "\n",
    "    # DEBUG\n",
    "    i = 0\n",
    "    for point in forward_points:\n",
    "        relative_x = int(point[0] * frame.shape[1])\n",
    "        relative_y = int(point[1] * frame.shape[0])\n",
    "\n",
    "        cv2.circle(annotated_image, (relative_x, relative_y), radius=1, color=(0, 0 + i, 0), thickness=7)\n",
    "        i+= 50\n",
    "\n",
    "    return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def overlay(image, filter_img, face_landmarks, face_part, INDEXES):\n",
    "    '''\n",
    "    This function will overlay a filter image over a face part of a person in the image/frame.\n",
    "    Args:\n",
    "        image:          The image of a person on which the filter image will be overlayed.\n",
    "        filter_img:     The filter image that is needed to be overlayed on the image of the person.\n",
    "        face_landmarks: The facial landmarks of the person in the image.\n",
    "        face_part:      The name of the face part on which the filter image will be overlayed.\n",
    "        INDEXES:        The indexes of landmarks of the face part.\n",
    "        display:        A boolean value that is if set to true the function displays \n",
    "                        the annotated image and returns nothing.\n",
    "    Returns:\n",
    "        annotated_image: The image with the overlayed filter on the top of the specified face part.\n",
    "    '''\n",
    "    \n",
    "    # Create a copy of the image to overlay filter image on.\n",
    "    annotated_image = image.copy()\n",
    "    \n",
    "    # Errors can come when it resizes the filter image to a too small or a too large size .\n",
    "    # So use a try block to avoid application crashing.\n",
    "    try:\n",
    "            \n",
    "        # Get the width and height of filter image.\n",
    "        filter_img_height, filter_img_width, _  = filter_img.shape\n",
    "        \n",
    "        # Get the width and height of the image\n",
    "        image_height, image_width, _ = image.shape\n",
    "\n",
    "        # Check if the face part is mouth.\n",
    "        if face_part == 'MOUTH':\n",
    "            \n",
    "            # Get the height of the face part on which we will overlay the filter image.\n",
    "            _, face_part_height, landmarks = getSize(image, face_landmarks, INDEXES)\n",
    "\n",
    "            # Specify the height to which the filter image is required to be resized.\n",
    "            # 2.5 can be changed depending on the size of the filter we want\n",
    "            # This allows the filter to be bigger/smaller depending on both the size of the eye and the size of the aperture\n",
    "            # of the eye\n",
    "            required_height = int(face_part_height*1.5)\n",
    "            required_width = int(filter_img_width*(required_height/filter_img_height))\n",
    "\n",
    "            #annotated_image = drawFilter(image, filter_img, face_landmarks, INDEXES, [required_width, required_height])\n",
    "\n",
    "        # Otherwise if the face part is an eye.\n",
    "        elif face_part == 'LEFT EYE' or face_part == 'RIGHT EYE':\n",
    "            \n",
    "            # Get the height of the face part on which we will overlay the filter image.\n",
    "            _, face_part_height, landmarks = getSize(image, face_landmarks, INDEXES)\n",
    "\n",
    "            # Specify the height to which the filter image is required to be resized.\n",
    "            # 2.5 can be changed depending on the size of the filter we want\n",
    "            # This allows the filter to be bigger/smaller depending on both the size of the eye and the size of the aperture\n",
    "            # of the eye\n",
    "            required_height = int(face_part_height*2.5)\n",
    "            required_width = int(filter_img_width*(required_height/filter_img_height))\n",
    "\n",
    "            #annotated_image = drawFilter(image, filter_img, face_landmarks, INDEXES, [required_width, required_height])\n",
    "            \n",
    "        elif face_part == 'FOREHEAD':\n",
    "                        \n",
    "            # Get the height of the face part on which we will overlay the filter image.\n",
    "            face_part_width, _, landmarks = getSize(image, face_landmarks, INDEXES)\n",
    "            \n",
    "            # Specify the height to which the filter image is required to be resized.\n",
    "            # 2.5 can be changed depending on the size of the filter we want\n",
    "            # This allows the filter to be bigger/smaller depending on both the size of the eye and the size of the aperture\n",
    "            # of the eye\n",
    "            required_width = int(face_part_width*2.5)\n",
    "            required_height = int(filter_img_height*(required_width/filter_img_width))\n",
    "            \n",
    "            #annotated_image = drawFilter(image, filter_img, face_landmarks, INDEXES, [required_width, required_height])\n",
    "    \n",
    "        annotated_image = drawFilter(image, filter_img, face_landmarks, INDEXES, [required_width, required_height])\n",
    "            \n",
    "            \n",
    "    # Catch and handle the error(s).\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "            \n",
    "    # Return the annotated image.\n",
    "    return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the mediapipe face mesh class.\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "# Setup the face landmarks function for videos.\n",
    "# TODO: Here works for only one face --> Make it work for multiple faces (see overlay call in main function)\n",
    "face_mesh_videos = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, \n",
    "                                         min_detection_confidence=0.5,min_tracking_confidence=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the mediapipe drawing class.\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize the mediapipe drawing styles class.\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the VideoCapture object to read from the webcam.\n",
    "camera_video = cv2.VideoCapture(0)\n",
    "\n",
    "# Set camera resolution\n",
    "camera_video.set(3,1280)\n",
    "camera_video.set(4,960)\n",
    "\n",
    "# Create named window for resizing purposes.\n",
    "cv2.namedWindow('Face Filter', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Read the left and right eyes images.\n",
    "# TODO: Create an interactive window to let the user select the wanted filters\n",
    "left_eye = cv2.imread('data/left_eye_cupcake.png')\n",
    "right_eye = cv2.imread('data/right_eye_cupcake.png')\n",
    "\n",
    "crown = cv2.imread('data/crown.png')\n",
    "\n",
    "# Initialize the VideoCapture object to read from the smoke animation video stored in the disk.\n",
    "# TODO: Same as for images --> Create a dictionary to store all images and videos?\n",
    "animation = cv2.VideoCapture('data/rainbow_animation1.mp4')\n",
    "\n",
    "# Set the smoke animation video frame counter to zero.\n",
    "animation_frame_counter = 0\n",
    "\n",
    "# Iterate until the webcam is accessed successfully.\n",
    "while camera_video.isOpened():\n",
    "    \n",
    "    # Read a frame.\n",
    "    ok, frame = camera_video.read()\n",
    "    \n",
    "    # Check if frame is not read properly then continue to the next iteration to read\n",
    "    # the next frame.\n",
    "    if not ok:\n",
    "        continue\n",
    "        \n",
    "    # Get the width and height of the image\n",
    "    image_height, image_width, _ = frame.shape\n",
    "        \n",
    "    # Read a frame from smoke animation video\n",
    "    _, animation_frame = animation.read()\n",
    "    \n",
    "    # Increment the smoke animation video frame counter.\n",
    "    animation_frame_counter += 1\n",
    "    \n",
    "    # Check if the current frame is the last frame of the animation video.\n",
    "    if animation_frame_counter == animation.get(cv2.CAP_PROP_FRAME_COUNT):     \n",
    "        \n",
    "        # Set the current frame position to first frame to restart the video.\n",
    "        animation.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "        \n",
    "        # Set the animation video frame counter to zero.\n",
    "        animation_frame_counter = 0\n",
    "    \n",
    "    # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Perform Face landmarks detection.\n",
    "    _, face_mesh_results = detectFacialLandmarks(frame, face_mesh_videos)\n",
    "    \n",
    "    \n",
    "    # Check if facial landmarks are found.\n",
    "    if face_mesh_results.multi_face_landmarks:        \n",
    "        # Get the mouth isOpen status of the person in the frame.\n",
    "        mouth_status = isOpen(frame, face_mesh_results, 'MOUTH', \n",
    "                                     threshold=15)\n",
    "        \n",
    "        # Get the left eye isOpen status of the person in the frame.\n",
    "        left_eye_status = isOpen(frame, face_mesh_results, 'LEFT EYE', \n",
    "                                        threshold=4.5)\n",
    "        \n",
    "        # Get the right eye isOpen status of the person in the frame.\n",
    "        right_eye_status = isOpen(frame, face_mesh_results, 'RIGHT EYE', \n",
    "                                         threshold=4.5)\n",
    "        \n",
    "        # Iterate over the found faces.\n",
    "        for face_num, face_landmarks in enumerate(face_mesh_results.multi_face_landmarks):\n",
    "            \n",
    "            # Check if the left eye of the face is open.\n",
    "            if left_eye_status[face_num] == 'OPEN':\n",
    "                \n",
    "                # Get the width and height of filter image.\n",
    "                filter_img_height, filter_img_width, _  = left_eye.shape\n",
    "                \n",
    "                # Convert the indexes of the landmarks of the face part into a list.\n",
    "                # TODO: Rewrite it in a more natural way OR undestand it + Is it necessaray to create it as a list?\n",
    "                left_eye_landmarks = set(list(itertools.chain(*mp_face_mesh.FACEMESH_LEFT_EYE)))\n",
    "                \n",
    "                            # Get the height of the face part on which we will overlay the filter image.\n",
    "                _, face_part_height, landmarks = getSize(frame, face_landmarks, left_eye_landmarks)\n",
    "\n",
    "                # Specify the height to which the filter image is required to be resized.\n",
    "                # 2.5 can be changed depending on the size of the filter we want\n",
    "                # This allows the filter to be bigger/smaller depending on both the size of the eye and the size of the aperture\n",
    "                # of the eye\n",
    "                required_height = int(face_part_height*2.5)\n",
    "                required_width = int(filter_img_width*(required_height/filter_img_height))\n",
    "                \n",
    "                frame = drawFilter(frame, left_eye, face_landmarks, \n",
    "                                   left_eye_landmarks, [required_width, required_height])\n",
    "                \n",
    "            \n",
    "            # Check if the right eye of the face is open.\n",
    "            if right_eye_status[face_num] == 'OPEN':\n",
    "                \n",
    "                # Get the width and height of filter image.\n",
    "                filter_img_height, filter_img_width, _  = right_eye.shape\n",
    "                \n",
    "                # Convert the indexes of the landmarks of the face part into a list.\n",
    "                # TODO: Rewrite it in a more natural way OR undestand it + Is it necessaray to create it as a list?\n",
    "                right_eye_landmarks = set(list(itertools.chain(*mp_face_mesh.FACEMESH_RIGHT_EYE)))\n",
    "                \n",
    "                # Get the height of the face part on which we will overlay the filter image.\n",
    "                _, face_part_height, landmarks = getSize(frame, face_landmarks, right_eye_landmarks)\n",
    "\n",
    "                # Specify the height to which the filter image is required to be resized.\n",
    "                # 2.5 can be changed depending on the size of the filter we want\n",
    "                # This allows the filter to be bigger/smaller depending on both the size of the eye and the size of the aperture\n",
    "                # of the eye\n",
    "                required_height = int(face_part_height*2.5)\n",
    "                required_width = int(filter_img_width*(required_height/filter_img_height))\n",
    "                \n",
    "                frame = drawFilter(frame, right_eye, face_landmarks, \n",
    "                                   right_eye_landmarks, [required_width, required_height])\n",
    "                            \n",
    "            # Check if the mouth of the face is open.\n",
    "            if mouth_status[face_num] == 'OPEN':\n",
    "                \n",
    "                # Get the width and height of filter image.\n",
    "                filter_img_height, filter_img_width, _  = animation_frame.shape\n",
    "                \n",
    "                # Convert the indexes of the landmarks of the face part into a list.\n",
    "                # TODO: Rewrite it in a more natural way OR undestand it + Is it necessaray to create it as a list?\n",
    "                lips_landmarks = set(list(itertools.chain(*mp_face_mesh.FACEMESH_LIPS)))\n",
    "                \n",
    "                # Get the height of the face part on which we will overlay the filter image.\n",
    "                face_part_width, face_part_height, landmarks = getSize(frame, face_landmarks, lips_landmarks)\n",
    "                \n",
    "                # Specify the height to which the filter image is required to be resized.\n",
    "                # 2.5 can be changed depending on the size of the filter we want\n",
    "                # This allows the filter to be bigger/smaller depending on both the size of the eye and the size of the aperture\n",
    "                # of the eye\n",
    "                #required_height = int(face_part_height*1.5)\n",
    "                #required_width = int(filter_img_width*(required_height/filter_img_height))\n",
    "                required_width = int(face_part_width*0.7)\n",
    "                required_height = int(filter_img_height*(required_width/filter_img_width))\n",
    "                \n",
    "                frame = drawFilter(frame, animation_frame, face_landmarks, \n",
    "                                   lips_landmarks, [required_width, required_height])\n",
    "            \n",
    "            # FOREHEAD\n",
    "            # Get the width and height of filter image.\n",
    "            filter_img_height, filter_img_width, _  = crown.shape\n",
    "            \n",
    "            forehead_landmarks = [103, 67, 109, 10, 338, 297, 332]\n",
    "            \n",
    "            # Get the height of the face part on which we will overlay the filter image.\n",
    "            face_part_width,_, landmarks = getSize(frame, face_landmarks, forehead_landmarks)\n",
    "                \n",
    "            # Specify the height to which the filter image is required to be resized.\n",
    "            # 2.5 can be changed depending on the size of the filter we want\n",
    "            # This allows the filter to be bigger/smaller depending on both the size of the eye and the size of the aperture\n",
    "            # of the eye\n",
    "            required_width = int(face_part_width*2.5)\n",
    "            required_height = int(filter_img_height*(required_width/filter_img_width))\n",
    "            \n",
    "            frame = drawFilter(frame, crown, face_landmarks, \n",
    "                                   forehead_landmarks, [required_width, required_height])\n",
    "        \n",
    "    \n",
    "    # Display the frame.\n",
    "    cv2.imshow('Face Filter', frame)\n",
    "    \n",
    "    # Wait for 1ms. If a key is pressed, retreive the ASCII code of the key.\n",
    "    k = cv2.waitKey(1) & 0xFF    \n",
    "    \n",
    "    # Check if 'ESC' is pressed and break the loop.\n",
    "    if(k == 27):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture Object and close the windows.                  \n",
    "camera_video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.00000000e+00  0.00000000e+00 -2.46139224e-14]\n",
      " [ 0.00000000e+00  1.00000000e+00 -1.47683534e-13]\n",
      " [ 0.00000000e+00  0.00000000e+00  1.00000000e+00]]\n",
      "463 396\n",
      "\n"
     ]
    }
   ],
   "source": [
    "right_eye = cv2.imread('data/right_eye_cupcake.png')\n",
    "\n",
    "filter_img_height, filter_img_width, _  = right_eye.shape\n",
    "\n",
    "pts1 = np.array([[0, 0], [0, filter_img_height], [filter_img_width, filter_img_height], [filter_img_width, 0]])\n",
    "pts2 = np.array([[0, 0], [0, filter_img_height], [filter_img_width, filter_img_height], [filter_img_width, 0]])\n",
    "\n",
    "h, mask = cv2.findHomography(pts1, pts2, cv2.RANSAC,5.0)\n",
    "\n",
    "print(h)\n",
    "\n",
    "print(filter_img_height, filter_img_width)\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 1)\n",
      "[[[1]\n",
      "  [2]]\n",
      "\n",
      " [[3]\n",
      "  [4]]]\n",
      "(2, 2, 3)\n",
      "[[[1 1 1]\n",
      "  [2 2 2]]\n",
      "\n",
      " [[3 3 3]\n",
      "  [4 4 4]]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[[1], [2]], [[3], [4]]])\n",
    "print(x.shape)\n",
    "print(x)\n",
    "\n",
    "\n",
    "x = np.repeat(x, 3, axis=2)\n",
    "print(x.shape)\n",
    "\n",
    "print(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF573",
   "language": "python",
   "name": "inf573"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
