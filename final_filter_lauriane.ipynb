{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import itertools\n",
    "import numpy as np\n",
    "from time import time\n",
    "import mediapipe as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import open3d as o3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSize(image, face_landmarks, INDEXES):\n",
    "    '''\n",
    "    This function calculates the height and width of a face part utilizing its landmarks.\n",
    "    Args:\n",
    "        image:          The image of person(s) whose face part size is to be calculated.\n",
    "        face_landmarks: The detected face landmarks of the person whose face part size is to \n",
    "                        be calculated.\n",
    "        INDEXES:        The indexes of the face part landmarks, whose size is to be calculated.\n",
    "    Returns:\n",
    "        width:     The calculated width of the face part of the face whose landmarks were passed.\n",
    "        height:    The calculated height of the face part of the face whose landmarks were passed.\n",
    "        landmarks: An array of landmarks of the face part whose size is calculated.\n",
    "    '''\n",
    "    \n",
    "    # Retrieve the height and width of the image.\n",
    "    image_height, image_width, _ = image.shape\n",
    "    \n",
    "    # Initialize a list to store the landmarks of the face part.\n",
    "    landmarks = []\n",
    "    \n",
    "    # Iterate over the indexes of the landmarks of the face part. \n",
    "    for INDEX in INDEXES:\n",
    "        \n",
    "        # Append the landmark into the list.\n",
    "        landmarks.append([int(face_landmarks.landmark[INDEX].x * image_width),\n",
    "                               int(face_landmarks.landmark[INDEX].y * image_height)])\n",
    "    \n",
    "    # Calculate the width and height of the face part.\n",
    "    # TODO: Use the 3D box instead of 2D rectangle\n",
    "    _, _, width, height = cv2.boundingRect(np.array(landmarks))\n",
    "    \n",
    "    # Convert the list of landmarks of the face part into a numpy array.\n",
    "    landmarks = np.array(landmarks)\n",
    "    \n",
    "    # Return the calculated width height and the landmarks of the face part.\n",
    "    return width, height, landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isOpen(image, face_mesh_results, face_part, threshold=5):\n",
    "    '''\n",
    "    This function checks whether the eye or mouth of the person(s) is open, \n",
    "    utilizing its facial landmarks.\n",
    "    Args:\n",
    "        image:             The image of person(s) whose an eye or mouth is to be checked.\n",
    "        face_mesh_results: The output of the facial landmarks detection on the image.\n",
    "        face_part:         The name of the face part that is required to check: MOUTH, RIGHT EYE, LEFT EYE\n",
    "        threshold:         The threshold value used to check the isOpen condition.\n",
    "    Returns:\n",
    "        status:       A dictionary containing isOpen statuses of the face part of all the \n",
    "                      detected faces.  \n",
    "    '''\n",
    "    \n",
    "    # Retrieve the height and width of the image.\n",
    "    image_height, image_width, _ = image.shape\n",
    "\n",
    "    \n",
    "    # Create a dictionary to store the isOpen status of the face part of all the detected faces.\n",
    "    status={}\n",
    "    \n",
    "    # Check if the face part is mouth.\n",
    "    if face_part == 'MOUTH':\n",
    "        \n",
    "        # Get the indexes of the mouth.\n",
    "        INDEXES = mp_face_mesh.FACEMESH_LIPS\n",
    "        \n",
    "    # Check if the face part is left eye.    \n",
    "    elif face_part == 'LEFT EYE':\n",
    "        \n",
    "        # Get the indexes of the left eye.\n",
    "        INDEXES = mp_face_mesh.FACEMESH_LEFT_EYE\n",
    "    \n",
    "    # Check if the face part is right eye.    \n",
    "    elif face_part == 'RIGHT EYE':\n",
    "        \n",
    "        # Get the indexes of the right eye.\n",
    "        INDEXES = mp_face_mesh.FACEMESH_RIGHT_EYE \n",
    "           \n",
    "    # Otherwise return nothing.\n",
    "    else:\n",
    "        return\n",
    "    \n",
    "    # Convert the indexes of the landmarks of the face part into a list of unique indices.\n",
    "    INDEXES_LIST = set(list(itertools.chain(*INDEXES)))\n",
    "    \n",
    "    # Iterate over the found faces.\n",
    "    for face_no, face_landmarks in enumerate(face_mesh_results.multi_face_landmarks):\n",
    "        \n",
    "        # Get the height of the face part.\n",
    "        _, height, _ = getSize(image, face_landmarks, INDEXES_LIST)\n",
    "        \n",
    "        # Get the height of the whole face.\n",
    "        face_oval = set(list(itertools.chain(*mp_face_mesh.FACEMESH_FACE_OVAL)))\n",
    "        _, face_height, _ = getSize(image, face_landmarks, face_oval)\n",
    "        \n",
    "        # Check if the face part is open.\n",
    "        if (height/face_height)*100 > threshold:\n",
    "            \n",
    "            # Set status of the face part to open.\n",
    "            status[face_no] = 'OPEN'\n",
    "        \n",
    "        # Otherwise.\n",
    "        else:\n",
    "            # Set status of the face part to close.\n",
    "            status[face_no] = 'CLOSE'\n",
    "    \n",
    "    # Otherwise\n",
    "    else:\n",
    "        \n",
    "        # Return the output image and the isOpen statuses of the face part of each detected face.\n",
    "        return status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkRotation(face_landmarks):\n",
    "    '''\n",
    "    This function returns the order of the forward rectangle points of the 3D bounding box of the face\n",
    "    Args:\n",
    "        face_landmarks: Landmarks of the face\n",
    "    Returns:\n",
    "        -:      The number represents the order of the forward rectangle points, which are the points 0, 1, 2 and 7 \n",
    "                of the array with the 8 points, in the oriented space\n",
    "                0: 0*****2    1: 7*****1    2: 2*****0    3: 1*****7    4: error\n",
    "                   *     *       *     *       *     *       *     *\n",
    "                   *     *       *     *       *     *       *     *\n",
    "                   1*****7       2*****0       7*****1       0*****2\n",
    "        R: Rotation matrix of the 3D bounding box\n",
    "        points: points of the 3D bounding box\n",
    "    '''\n",
    "    \n",
    "    # Retrieve the landmarks of the face as a list\n",
    "    face_landmarks_l = []\n",
    "    for index in range(len(face_landmarks.landmark)):\n",
    "        face_landmarks_l.append([face_landmarks.landmark[index].x, \n",
    "                          face_landmarks.landmark[index].y, \n",
    "                          face_landmarks.landmark[index].z])\n",
    "    \n",
    "    # Get the 3D oriented bounding box\n",
    "    o3d_landmarks_f = o3d.utility.Vector3dVector(np.array(face_landmarks_l))\n",
    "    o3d_bbox_f = o3d.geometry.OrientedBoundingBox.create_from_points(o3d_landmarks_f)\n",
    "    \n",
    "    # Get the rotation matrix from the oriented space to the aligned space\n",
    "    R = o3d_bbox_f.R\n",
    "    points = np.asarray(o3d_bbox_f.get_box_points())\n",
    "    \n",
    "    # In the aligned space the points are stored like this\n",
    "    #  2*****7\n",
    "    #  *     *\n",
    "    #  *     *\n",
    "    #  0*****1\n",
    "    # So we check how they turned from oriented space to aligned space to get the rotation/inversion\n",
    "    if points[2, 0] > points[1, 0] and points[2, 1] > points[1, 1]:\n",
    "        return 0, R, points\n",
    "    elif points[2, 0] < points[1, 0] and points[2, 1] < points[1, 1]:\n",
    "        return 1, R, points\n",
    "    elif points[2, 0] < points[1, 0] and points[2, 1] > points[1, 1]:\n",
    "        return 2, R, points\n",
    "    elif points[2, 0] > points[1, 0] and points[2, 1] < points[1, 1]:\n",
    "        return 3, R, points\n",
    "    else:\n",
    "        return 4, R, points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boundingBox(face_landmarks, face_part, INDEXES, angle, R, required_size):\n",
    "    '''\n",
    "    This function returns the new corner points of the filter image in the image space\n",
    "    Args:\n",
    "        face_landmarks: Landmarks of the face\n",
    "        face_part:      Name of the face part we want to apply the filter on\n",
    "        INDEXES:        Indices of the face part landmarks\n",
    "        angle:          Orientation of the 3D bouding box of the entire face\n",
    "        R:              The rotation matrix of the 3D bounding box\n",
    "        required_size:  Required width and height of the filter in the aligned space = before applying persective\n",
    "    Returns:\n",
    "        forward_points: New corner points of the filter image in the oriented space\n",
    "    '''\n",
    "    \n",
    "    # Retrieve the landmarks of the face part\n",
    "    landmarks = []\n",
    "    for index in INDEXES:\n",
    "        landmarks.append([face_landmarks.landmark[index].x, \n",
    "                          face_landmarks.landmark[index].y, \n",
    "                          face_landmarks.landmark[index].z])\n",
    "    \n",
    "    # Create the 3D oriented bounding box\n",
    "    o3d_landmarks = o3d.utility.Vector3dVector(np.array(landmarks))\n",
    "    o3d_bbox = o3d.geometry.OrientedBoundingBox.create_from_points(o3d_landmarks)\n",
    "    \n",
    "    # Get box points and center point\n",
    "    box_points = np.asarray(o3d_bbox.get_box_points())\n",
    "    box_center = np.asarray(o3d_bbox.get_center())\n",
    "\n",
    "    # Retrieve the inverse rotation matrix of the oriented bounding box --> R_inv put a point in the oriented space\n",
    "    # to the aligned space\n",
    "    R_inv = np.linalg.inv(R)\n",
    "    \n",
    "    # Retrieve one of the forward points to get the depth in the aligned space\n",
    "    index = np.argmin(box_points[:,2])\n",
    "    forward_point = box_points[index, :] \n",
    "    \n",
    "    # Project the point in the aligned space and center\n",
    "    forward_point_projected = np.matmul(R_inv, forward_point - box_center)\n",
    "    \n",
    "    # Get the required width and height\n",
    "    required_width, required_height = required_size\n",
    "    \n",
    "\n",
    "    # Get the points position in aligned space in the right order\n",
    "    # It depends on the filter type and orientation of the bounding box\n",
    "    # We are in the aligned space and we will then put the image in the oriented space and then in the image space.\n",
    "    # We want the points such that we have 0*****3 in image space, the number being the order in which they are arranged\n",
    "    #                                      *     * \n",
    "    #                                      *     *\n",
    "    #                                      1*****2\n",
    "    # If angle == 0\n",
    "    # In aligned space we have 2*****7, in oriented space 0*****2, and in image space 0*****1 for the bounding box points\n",
    "    #                          *     *                    *     *                     *     *\n",
    "    #                          *     *                    *     *                     *     *\n",
    "    #                          0*****1                    1*****7                     2*****7\n",
    "    #\n",
    "    # So the the points should be written 2*****3\n",
    "    #                                     *     *\n",
    "    #                                     *     *\n",
    "    #                                     1*****0\n",
    "    # That way we will have \n",
    "    # In aligned space 2*****3, in oriented space 1*****2, and in image space 0*****3\n",
    "    #                  *     *                    *     *                     *     *\n",
    "    #                  *     *                    *     *                     *     *\n",
    "    #                  1*****0                    0*****3                     1*****2\n",
    "    forward_points_transformed = []\n",
    "    if face_part == 'FOREHEAD':        \n",
    "        if angle == 0:            \n",
    "            forward_points_transformed.append([required_height, -required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([0, -required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([0, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([required_height, required_width / 2, forward_point_projected[2]])\n",
    "        elif angle == 1:\n",
    "            forward_points_transformed.append([-required_height, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([0, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([0, - required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([-required_height, - required_width / 2, forward_point_projected[2]])\n",
    "        elif angle == 2:\n",
    "            forward_points_transformed.append([required_height, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([0, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([0, - required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([required_height, - required_width / 2, forward_point_projected[2]])\n",
    "        elif angle == 3:\n",
    "            forward_points_transformed.append([-required_height, -required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([0, -required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([0, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([-required_height, required_width / 2, forward_point_projected[2]])\n",
    "        else:\n",
    "            return\n",
    "\n",
    "            \n",
    "    elif face_part == 'RIGHT EYE' or face_part == 'LEFT EYE':  \n",
    "        if angle == 0:            \n",
    "            forward_points_transformed.append([required_height/2, -required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([-required_height/2, -required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([-required_height/2, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([required_height/2, required_width / 2, forward_point_projected[2]])\n",
    "        elif angle == 1:\n",
    "            forward_points_transformed.append([-required_height/2, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([required_height/2, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([required_height/2, - required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([-required_height/2, - required_width / 2, forward_point_projected[2]])\n",
    "        elif angle == 2:\n",
    "            forward_points_transformed.append([required_height/2, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([-required_height/2, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([-required_height/2, - required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([required_height/2, - required_width / 2, forward_point_projected[2]])\n",
    "        elif angle == 3:\n",
    "            forward_points_transformed.append([-required_height/2, -required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([required_height/2, -required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([required_height/2, required_width / 2, forward_point_projected[2]])\n",
    "            forward_points_transformed.append([-required_height/2, required_width / 2, forward_point_projected[2]])\n",
    "        else:\n",
    "            return\n",
    "        \n",
    "    elif face_part == 'MOUTH':\n",
    "        if angle == 0:            \n",
    "            forward_points_transformed.append([0, -required_width / 2, 0])\n",
    "            forward_points_transformed.append([-required_height, -required_width / 2, 0])\n",
    "            forward_points_transformed.append([-required_height, required_width / 2, 0])\n",
    "            forward_points_transformed.append([0, required_width / 2, 0])\n",
    "        elif angle == 1:\n",
    "            forward_points_transformed.append([0, required_width / 2, 0])\n",
    "            forward_points_transformed.append([required_height, required_width / 2, 0])\n",
    "            forward_points_transformed.append([required_height, - required_width / 2, 0])\n",
    "            forward_points_transformed.append([0, - required_width / 2, 0])\n",
    "        elif angle == 2:\n",
    "            forward_points_transformed.append([0, required_width / 2, 0])\n",
    "            forward_points_transformed.append([-required_height, required_width / 2, 0])\n",
    "            forward_points_transformed.append([-required_height, - required_width / 2, 0])\n",
    "            forward_points_transformed.append([0, - required_width / 2, 0])\n",
    "        elif angle == 3:\n",
    "            forward_points_transformed.append([0, -required_width / 2, 0])\n",
    "            forward_points_transformed.append([required_height, -required_width / 2, 0])\n",
    "            forward_points_transformed.append([required_height, required_width / 2, 0])\n",
    "            forward_points_transformed.append([0, required_width / 2, 0])\n",
    "        else:\n",
    "            return     \n",
    "    else:\n",
    "        return\n",
    "       \n",
    "    # Get the filter image in the oriented space \n",
    "    forward_points = []\n",
    "    for p in forward_points_transformed:\n",
    "        p_rotated = np.matmul(R, p) # Rotate\n",
    "        p_translated = p_rotated + box_center # Translate to box position\n",
    "        forward_points.append(p_translated)\n",
    "            \n",
    "    return forward_points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drawFilter(image, filter_img, face_landmarks, face_part, INDEXES, angle, R, required_size):\n",
    "    '''\n",
    "    This function returns camera image with the filter image added at the right place\n",
    "    Args:\n",
    "        image:           Camera image\n",
    "        filter_image:    Filter image\n",
    "        face_landmarks:  Landmarks of the entire face\n",
    "        face_part:       The name of the part you want to put the filter on\n",
    "        INDEXES:         Indices of the face part landmarks\n",
    "        angle:           Orientation of the 3D bouding box of the entire face\n",
    "        R:               The rotation matrix of the 3D bounding box\n",
    "        required_size:   Required width and height of the filter in the aligned space = before applying persective\n",
    "    Returns:\n",
    "        annotated_image: The image with the filter added\n",
    "    '''\n",
    "    \n",
    "    # Get width and height of the filter image\n",
    "    filter_img_height, filter_img_width, _  = filter_img.shape\n",
    "    \n",
    "    # Get width and height of the camera image\n",
    "    image_height, image_width, _ = image.shape\n",
    "    \n",
    "    # Get required width and height\n",
    "    required_width = required_size[0]\n",
    "    required_height = required_size[1]\n",
    "    \n",
    "    # Normalize the required width and height for aligned/oriented spaces\n",
    "    required_height_norm = required_height / image_height\n",
    "    required_width_norm = required_width / image_width\n",
    "    \n",
    "    # Get the new corners points of the filter image (in the oriented space)\n",
    "    forward_points = boundingBox(face_landmarks, face_part, INDEXES, angle, R,\n",
    "                                                             [required_width_norm, required_height_norm])\n",
    "          \n",
    "    # Get the initial corner points of the filter image (counterclockwise) in image space\n",
    "    # 0*****3\n",
    "    # *     *\n",
    "    # *     *\n",
    "    # 1*****2\n",
    "    pts1 = np.array([[0, 0], [0, filter_img_height], \n",
    "                     [filter_img_width, filter_img_height], [filter_img_width, 0]])\n",
    "    \n",
    "    # Get the final corner points of the filter image in the overlay image (counterclockwise) in image space\n",
    "    # with perspective\n",
    "    pts2 = []\n",
    "    for point in forward_points:\n",
    "        resolution_x = 1\n",
    "        resolution_y = 1\n",
    "\n",
    "        # Add perspective\n",
    "        fov = 45\n",
    "        z0 = (resolution_x / 2) / np.tan((fov/2) * np.pi / 180)\n",
    "\n",
    "        relative_x = int(((point[0] - 0.5) * z0 / (z0 + point[2]) + 0.5) * image_width)\n",
    "        relative_y = int(((point[1] - 0.5) * z0 / (z0 + point[2]) + 0.5) * image_height)\n",
    "\n",
    "        pts2.append([relative_x, relative_y])\n",
    "    pts2 = np.array(pts2)\n",
    "\n",
    "    # Find homography from the filter image to the overlay image\n",
    "    h, mask = cv2.findHomography(pts1, pts2, cv2.RANSAC,5.0)\n",
    "\n",
    "    # Put the filter image in perspective\n",
    "    im1Reg = cv2.warpPerspective(filter_img, h, (image_width, image_height))\n",
    "\n",
    "    # Inverse filter image mask in perspective\n",
    "    _, filter_img_mask = cv2.threshold(cv2.cvtColor(im1Reg, cv2.COLOR_BGR2GRAY),\n",
    "                                   25, 255, cv2.THRESH_BINARY_INV)\n",
    "\n",
    "    # Spread in 3 dimensions (rgb)\n",
    "    filter_img_mask = np.expand_dims(filter_img_mask, axis=2)\n",
    "    filter_img_mask = np.repeat(filter_img_mask, 3, axis=2)\n",
    "\n",
    "    # Final image with black pixels at filter image position\n",
    "    resultant_image = cv2.bitwise_and(image, filter_img_mask)\n",
    "\n",
    "    # Use Bitwise or to merge the two images\n",
    "    annotated_image = cv2.bitwise_or(im1Reg, resultant_image)        \n",
    "\n",
    "    return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the mediapipe face mesh class.\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "# Setup the face landmarks function for videos.\n",
    "# TODO: Here works for only one face --> Make it work for multiple faces (see overlay call in main function)\n",
    "face_mesh_videos = mp_face_mesh.FaceMesh(static_image_mode=False, max_num_faces=1, \n",
    "                                         min_detection_confidence=0.5,min_tracking_confidence=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the mediapipe drawing class.\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "# Initialize the mediapipe drawing styles class.\n",
    "mp_drawing_styles = mp.solutions.drawing_styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Initialize the VideoCapture object to read from the webcam.\n",
    "camera_video = cv2.VideoCapture(0)\n",
    "\n",
    "# Set camera resolution\n",
    "camera_video.set(3,1280)\n",
    "camera_video.set(4,960)\n",
    "\n",
    "# Create named window for resizing purposes.\n",
    "cv2.namedWindow('Face Filter', cv2.WINDOW_NORMAL)\n",
    "\n",
    "# Read the left and right eyes images.\n",
    "# TODO: Create an interactive window to let the user select the wanted filters\n",
    "left_eye = cv2.imread('data/left_eye_cupcake.png')\n",
    "right_eye = cv2.imread('data/right_eye_cupcake.png')\n",
    "\n",
    "crown = cv2.imread('data/crown.png')\n",
    "\n",
    "# Initialize the VideoCapture object to read from the smoke animation video stored in the disk.\n",
    "# TODO: Same as for images --> Create a dictionary to store all images and videos?\n",
    "animation = cv2.VideoCapture('data/rainbow_animation1.mp4')\n",
    "\n",
    "# Set the smoke animation video frame counter to zero.\n",
    "animation_frame_counter = 0\n",
    "\n",
    "# Iterate until the webcam is accessed successfully.\n",
    "while camera_video.isOpened():\n",
    "    \n",
    "    # Read a frame.\n",
    "    ok, frame = camera_video.read()\n",
    "    \n",
    "    # Check if frame is not read properly then continue to the next iteration to read\n",
    "    # the next frame.\n",
    "    if not ok:\n",
    "        continue\n",
    "        \n",
    "    # Get the width and height of the image\n",
    "    image_height, image_width, _ = frame.shape\n",
    "        \n",
    "    # Read a frame from smoke animation video\n",
    "    _, animation_frame = animation.read()\n",
    "    \n",
    "    # Increment the smoke animation video frame counter.\n",
    "    animation_frame_counter += 1\n",
    "    \n",
    "    # Check if the current frame is the last frame of the animation video.\n",
    "    if animation_frame_counter == animation.get(cv2.CAP_PROP_FRAME_COUNT):     \n",
    "        \n",
    "        # Set the current frame position to first frame to restart the video.\n",
    "        animation.set(cv2.CAP_PROP_POS_FRAMES, 0)\n",
    "        \n",
    "        # Set the animation video frame counter to zero.\n",
    "        animation_frame_counter = 0\n",
    "    \n",
    "    # Flip the frame horizontally for natural (selfie-view) visualization.\n",
    "    frame = cv2.flip(frame, 1)\n",
    "    \n",
    "    # Perform Face landmarks detection.\n",
    "    face_mesh_results = face_mesh_videos.process(frame[:,:,::-1])    \n",
    "    \n",
    "    # Check if facial landmarks are found.\n",
    "    if face_mesh_results.multi_face_landmarks:        \n",
    "        # Get the mouth isOpen status of the person in the frame.\n",
    "        mouth_status = isOpen(frame, face_mesh_results, 'MOUTH', \n",
    "                                     threshold=15)\n",
    "        \n",
    "        # Get the left eye isOpen status of the person in the frame.\n",
    "        left_eye_status = isOpen(frame, face_mesh_results, 'LEFT EYE', \n",
    "                                        threshold=4.5)\n",
    "        \n",
    "        # Get the right eye isOpen status of the person in the frame.\n",
    "        right_eye_status = isOpen(frame, face_mesh_results, 'RIGHT EYE', \n",
    "                                         threshold=4.5)\n",
    "        \n",
    "        # Iterate over the found faces.\n",
    "        for face_num, face_landmarks in enumerate(face_mesh_results.multi_face_landmarks):\n",
    "            \n",
    "            # Check the orientation of the face bounding box\n",
    "            angle, R, points = checkRotation(face_landmarks)\n",
    "            \n",
    "            # Check if the left eye of the face is open.\n",
    "            if left_eye_status[face_num] == 'OPEN':\n",
    "                \n",
    "                # Get the width and height of filter image.\n",
    "                filter_img_height, filter_img_width, _  = left_eye.shape\n",
    "                \n",
    "                # Convert the indexes of the landmarks of the face part into a list.\n",
    "                left_eye_landmarks = set(list(itertools.chain(*mp_face_mesh.FACEMESH_LEFT_EYE)))\n",
    "                \n",
    "                # Get the height of the face part on which we will overlay the filter image.\n",
    "                _, face_part_height, landmarks = getSize(frame, face_landmarks, left_eye_landmarks)\n",
    "\n",
    "                # Specify the height to which the filter image is required to be resized.\n",
    "                # 2.5 can be changed depending on the size of the filter we want\n",
    "                # This allows the filter to be bigger/smaller depending on both the size of the eye \n",
    "                # and the size of the aperture of the eye\n",
    "                required_height = int(face_part_height*2.5)\n",
    "                required_width = int(filter_img_width*(required_height/filter_img_height))\n",
    "                \n",
    "                # Draw the filter\n",
    "                frame = drawFilter(frame, left_eye, face_landmarks, 'LEFT EYE',\n",
    "                                   left_eye_landmarks, angle, R, [required_width, required_height])\n",
    "                \n",
    "            \n",
    "            # Check if the right eye of the face is open.\n",
    "            if right_eye_status[face_num] == 'OPEN':\n",
    "                \n",
    "                # Get the width and height of filter image.\n",
    "                filter_img_height, filter_img_width, _  = right_eye.shape\n",
    "                \n",
    "                # Convert the indexes of the landmarks of the face part into a list.\n",
    "                right_eye_landmarks = set(list(itertools.chain(*mp_face_mesh.FACEMESH_RIGHT_EYE)))\n",
    "                \n",
    "                # Get the height of the face part on which we will overlay the filter image.\n",
    "                _, face_part_height, landmarks = getSize(frame, face_landmarks, right_eye_landmarks)\n",
    "\n",
    "                # Specify the height to which the filter image is required to be resized.\n",
    "                # 2.5 can be changed depending on the size of the filter we want\n",
    "                # This allows the filter to be bigger/smaller depending on both the size of the eye \n",
    "                # and the size of the aperture of the eye\n",
    "                required_height = int(face_part_height*2.5)\n",
    "                required_width = int(filter_img_width*(required_height/filter_img_height))\n",
    "                \n",
    "                # Draw the filter\n",
    "                frame = drawFilter(frame, right_eye, face_landmarks, 'RIGHT EYE',\n",
    "                                   right_eye_landmarks, angle, R, [required_width, required_height])\n",
    "                            \n",
    "            # Check if the mouth of the face is open.\n",
    "            if mouth_status[face_num] == 'OPEN':\n",
    "                \n",
    "                # Get the width and height of filter image.\n",
    "                filter_img_height, filter_img_width, _  = animation_frame.shape\n",
    "                \n",
    "                # Convert the indexes of the landmarks of the face part into a list.\n",
    "                lips_landmarks = set(list(itertools.chain(*mp_face_mesh.FACEMESH_LIPS)))\n",
    "                \n",
    "                # Get the height of the face part on which we will overlay the filter image.\n",
    "                face_part_width, face_part_height, landmarks = getSize(frame, face_landmarks, lips_landmarks)\n",
    "                \n",
    "                # Specify the height to which the filter image is required to be resized.\n",
    "                # 0.7 can be changed depending on the size of the filter we want\n",
    "                required_width = int(face_part_width*0.7)\n",
    "                required_height = int(filter_img_height*(required_width/filter_img_width))\n",
    "                \n",
    "                # Draw the filter\n",
    "                frame = drawFilter(frame, animation_frame, face_landmarks, 'MOUTH', \n",
    "                                   lips_landmarks, angle, R, [required_width, required_height])\n",
    "            \n",
    "            # FOREHEAD\n",
    "            # Get the width and height of filter image.\n",
    "            filter_img_height, filter_img_width, _  = crown.shape\n",
    "            \n",
    "            # Get the landmarks of the forehead\n",
    "            forehead_landmarks = [103, 67, 109, 10, 338, 297, 332]\n",
    "            \n",
    "            # Get the height of the face part on which we will overlay the filter image.\n",
    "            face_part_width,_, landmarks = getSize(frame, face_landmarks, forehead_landmarks)\n",
    "                \n",
    "            # Specify the height to which the filter image is required to be resized.\n",
    "            # 2.5 can be changed depending on the size of the filter we want\n",
    "            required_width = int(face_part_width*2.5)\n",
    "            required_height = int(filter_img_height*(required_width/filter_img_width))\n",
    "            \n",
    "            # Draw the filter\n",
    "            frame = drawFilter(frame, crown, face_landmarks, 'FOREHEAD',\n",
    "                                   forehead_landmarks, angle, R, [required_width, required_height])\n",
    "            \n",
    "\n",
    "        \n",
    "    \n",
    "    # Display the frame.\n",
    "    cv2.imshow('Face Filter', frame)\n",
    "    \n",
    "    # Wait for 1ms. If a key is pressed, retreive the ASCII code of the key.\n",
    "    k = cv2.waitKey(1) & 0xFF    \n",
    "    \n",
    "    # Check if 'ESC' is pressed and break the loop.\n",
    "    if(k == 27):\n",
    "        break\n",
    "\n",
    "# Release the VideoCapture Object and close the windows.                  \n",
    "camera_video.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "INF573",
   "language": "python",
   "name": "inf573"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
